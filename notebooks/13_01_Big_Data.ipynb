{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "  1. [Big Data & Hadoop](#Big_Data_Hadoop)\n",
    "  1. [HDFS](#HDFS)\n",
    "  1. [MapReduce](#MapReduce)\n",
    "  1. [Apache Spark](#Apache_Spark)\n",
    "2. [Map & Reduce](#Map_Reduce)\n",
    "3. [RDD](#RDD)\n",
    "4. [Key-Value RDD](#Key_Value_RDD)\n",
    "5. [DataFrame](#DataFrame)\n",
    "  1. [Pandas-like interface](#Pandas_interface)\n",
    "  2. [SQL interface](#SQL_interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<a id='Big_Data_Hadoop'></a>\n",
    "\n",
    "![b](http://www.kdnuggets.com/images/cartoon-make-data-great-again.jpg)\n",
    "\n",
    "## Big data & Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was a time when a researcher could gather all available data in their field of knowledge in a small library at home and produce results using a pen and a sheet of paper. With personal computers and laptops we have been able to extend our storage and processing power farther than we ever expected, but they cannot cope with it anymore.\n",
    "\n",
    "Nowadays, scientific experiments generate such amounts of data that they don't fit in a personal computer, not even in a data center such as PIC. This huge need of computing and storage resources is one of the factors that drive the scientific collaborations worldwide. Also, this dramatic increase in capacity and performance that is needed for current experiments requires specific architectures to store and process all this data.\n",
    "\n",
    "Big Data platforms are a combination of hardware and software designed to handle massive amounts of data. The most popular one is Hadoop. Hadoop is based on the design originally published by Google in several papers comprising, among others, of a:\n",
    " - distributed file system (HDFS)\n",
    " - MapReduce programming model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hadoop Distributed File System (HDFS) is the basis of the Hadoop platform, and it is built to work on top of commodity computer clusters. In this architecture, dozens up to thousands of cheap computers work in a coordinate manner to store and process the data. Due to the large number of elements involved (computer components, network, power, etc.) the platform was designed from the ground up to be failure tolerant. Should any element fail at any time, the system would detect the condition and recover from it transparently, and the user will not ever notice.\n",
    "\n",
    "HDFS works by splitting the files in 128 MiB blocks and replicating them on the cluster nodes in such a way that if a node fails, data is still accessible from any other replica.\n",
    "\n",
    "![HDFS](../resources/hdfs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce is programming model used for generating and processing big data sets with parallel and distributed algorithms. Inspired by the map and reduce functions commonly used in functional programming, its key contribution is the scalability and fault-tolerance achieved by optimizing the execution engine.\n",
    "\n",
    "In MapReduce, data operations are defined with respect to data structured in (key, value) pairs:\n",
    " - `Map` takes one pair of data in one data domain and returns a list of pairs in a different domain:\n",
    "       Map(k1,v1) → list(k2,v2)\n",
    "   The Map function is applied in parallel to every pair (keyed by k1) in the input dataset. This produces a list of pairs (keyed by k2) for each call. After that, the MapReduce framework collects all pairs with the same key (k2) from all lists and groups them together, creating one group for each key.\n",
    "\n",
    "\n",
    " - `Reduce` is then applied in parallel to each group, which in turn produces a collection of values in the same domain:\n",
    "       Reduce(k2, list (v2)) → list(v3)\n",
    "   Each Reduce call typically produces either one value v3 or an empty return, though one call is allowed to return more than one value. The returns of all calls are collected as the desired result list.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Apache_Spark'></a>\n",
    "## Apache Spark\n",
    "\n",
    "Is an open-source cluster-computing framework that can run on top of Apache Hadoop. Built on top of MapReduce, if offers an improved interface for non-linear algorithms and operations. Apache Spark is based on a specialized data structure called the resilient distributed dataset (RDD). The use of RDDs facilitates the implementation of iterative algorithms and interactive/exploratory analysis. The latency of Spark applications, compared to a pure MapReduce implementation, may be reduced by several orders of magnitude.\n",
    "\n",
    "Apache Spark comprises several modules which implement additional processing abilities to the RDDs such as:\n",
    " - Spark SQL: structured data like database result sets\n",
    " - Spark Streaming: real-time data\n",
    " - Spark MLlib: machine learning\n",
    " - Spark Graphx: graph processing\n",
    "\n",
    "For this course, we will introduce the mechanics of working with large datasets using Spark. Ideally, each one of you would have a entire Hadoop cluster to work with but, we are not CERN... so we make use of the ability of Spark to run locally, without a cluster. Later, you could run the same code on top of a Hadoop cluster without changing anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Map_Reduce'></a>\n",
    "# Map & Reduce\n",
    "\n",
    "![a](https://cdn.datafloq.com/cms/2015/03/19/big-data-cartoon.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "Spark operations can be classified as either:\n",
    " - ACTIONS: Trigger a computation and return a result\n",
    "    - reduce, collect, aggregate, groupBy, take, ...\n",
    " - TRANSFORMATIONS: return a new RDD with the transformation applied (think of composing functions)\n",
    "    - map, reduce, filter, join, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our input\n",
    "l = range(10)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We \"upload\" it as an RDD\n",
    "rdd = sc.parallelize(l)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a map function\n",
    "def power_of_2(k):\n",
    "    return 2**k\n",
    "\n",
    "# And we apply it to our RDD\n",
    "rdd.map(power_of_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we use collect() to retrieve all results.\n",
    "rdd.map(power_of_2).collect()\n",
    "\n",
    "### WARNING ###\n",
    "# Never do that in real cases, or you will transfer ALL data to your browser, effectibly killing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about summing, everything?\n",
    "# We define a reduce function\n",
    "def sum_everything(k1, k2):\n",
    "    return k1 + k2\n",
    "\n",
    "# And we apply the reduce operation\n",
    "rdd.reduce(sum_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can use the built in operation `sum`\n",
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if I wanted to compute the sum of the powers of 2?\n",
    "rdd.map(power_of_2).reduce(sum_everything)\n",
    "# or \n",
    "rdd.map(power_of_2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can we count the number of elements in the array?\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, too easy, this is supposed to be a map & reduce tutorial...\n",
    "\n",
    "How can we do it WITHOUT the count() action, just using map & reduce.\n",
    "\n",
    "**SPOILER**, you could add 1 for each element in the RDD:\n",
    " - Build a map function that given an element, it transforms it into a 1.\n",
    " - Then apply our `sum_everything` reduce function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_to_1(k):\n",
    "    return 1\n",
    "\n",
    "rdd.map(set_to_1).reduce(sum_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load all Shakespeare works\n",
    "import os\n",
    "shakespeare = sc.textFile(os.path.normpath('file:///../../resources/shakespeare.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first lines\n",
    "shakespeare.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the longest line\n",
    "def keep_longest(k1, k2):\n",
    "    if len(k1) > len(k2):\n",
    "        return k1\n",
    "    else:\n",
    "        return k2\n",
    "\n",
    "shakespeare.reduce(keep_longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average line length\n",
    "def line_length(k):\n",
    "    return len(k)\n",
    "\n",
    "shakespeare.map(line_length).sum() / shakespeare.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap() vs map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text in words\n",
    "def split_in_words(k):\n",
    "    return k.split()\n",
    "\n",
    "shakespeare.map(split_in_words).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare.flatMap(split_in_words).take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare.flatMap(\n",
    "    lambda k: k.split() # Split in words\n",
    ").take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve 10 words longer than 15 characters\n",
    "shakespeare.flatMap(\n",
    "    lambda k: k.split() # Split in words\n",
    ").filter(\n",
    "    lambda k: len(k)>15 # Keep words longer than 15 characters\n",
    ").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "How many times did use the word 'murder'? (**case insensitive**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -r 1-9 solutions/13_01_Big_Data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Show 10 words longer than 15 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -r 10-19 solutions/13_01_Big_Data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Show all words longer than 15 characters, but dropping those with any of the following characters (`. , -`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -r 20-29 solutions/13_01_Big_Data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Retrieve the longest word (without `. , -`), reusing the `keep_longest` reduce function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -r 30-39 solutions/13_01_Big_Data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which, as you all know, means \"the state of being able to achieve honours\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Key_Value_RDD'></a>\n",
    "# Key-Value RDD\n",
    "\n",
    "We want to count the number of appearances of every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = shakespeare.flatMap(\n",
    "    lambda k: k.split()                 # Split in words\n",
    ").filter(\n",
    "    lambda k: not (set('.,-') & set(k)) # Drop words with special characters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.groupBy(lambda k: k).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That method returns an iterable for each different word. This iterable contains a list of all the appearances of the word.\n",
    "# Lets print its contents\n",
    "\n",
    "tuples = words.groupBy(lambda k: k).take(5)\n",
    "\n",
    "for t in tuples:\n",
    "    print(t[0], list(t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, to compute the number of appearances, we just have to count the elements in the iterator\n",
    "\n",
    "words.groupBy(\n",
    "    lambda k: k\n",
    ").map(\n",
    "    lambda t: (t[0], len(list(t[1])))\n",
    ").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# But this is VERY EXPENSIVE in terms of memory,\n",
    "# as all the word instances must be stored in a list before they can be counted.\n",
    "\n",
    "# We can do it much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.map(\n",
    "    lambda w: (w, 1)\n",
    ").take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.map(\n",
    "    lambda w: (w, 1)\n",
    ").reduceByKey(\n",
    "    lambda k1, k2: k1 + k2 \n",
    ").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Get the 10 most-used words and its number of appearances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -r 40-49 solutions/13_01_Big_Data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Print then 10 most used words longer than 5 characters (case-insensitive)\n",
    "2. How many words, longer than 50 characters, are used more than 500 times? (case-insensitive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -r 50-69 solutions/13_01_Big_Data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -r 70-79 solutions/13_01_Big_Data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia = sqlc.read.csv('../resources/gaia.csv.bz2', comment='#', header=True, inferSchema=True)\n",
    "gaia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Pandas_interface'></a>\n",
    "## Pandas-like interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_hist = gaia.groupBy(\n",
    "    (\n",
    "        func.floor(gaia.mag_g * 10) / 10\n",
    "    ).alias('mag_g'),\n",
    ").count().orderBy(\n",
    "    'mag_g'\n",
    ")\n",
    "g_hist.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_hist.toPandas().set_index('mag_g').plot(loglog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Plot an 'ra' histogram in 1-degree bins (count how many stars are in each bin).\n",
    "\n",
    "Can you spot the galaxy center? ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -r 90-99 solutions/13_01_Big_Data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SQL_interface'></a>\n",
    "## SQL interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlc.registerDataFrameAsTable(gaia, \"gaia\")\n",
    "\n",
    "g_hist = sqlc.sql(\"\"\"\n",
    "    SELECT CAST(FLOOR(mag_g*10)/10. AS FLOAT) AS mag_g, COUNT(*) AS `count`\n",
    "    FROM gaia\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1\n",
    "\"\"\")\n",
    "\n",
    "g_hist.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_hist.toPandas().set_index('mag_g').plot(loglog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Plot an 'ra' histogram in 1-degree bins (count how many stars are in each bin).\n",
    "\n",
    "Can you spot the galaxy center? ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -r 100-109 solutions/13_01_Big_Data.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
