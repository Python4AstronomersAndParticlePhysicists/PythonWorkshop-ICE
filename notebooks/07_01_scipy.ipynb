{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy\n",
    "![SciPy](https://raw.githubusercontent.com/scipy/scipy-sphinx-theme/master/_static/scipyshiny_small.png)\n",
    "\n",
    "- Uses numpy as its core\n",
    "- Numerical methods for:\n",
    "    + integration\n",
    "    + solving differential equations\n",
    "    + optimizing, minimizing \n",
    "    + root finding\n",
    "    + fast fourier transforms\n",
    "- Contains the CODATA values for many constants of nature\n",
    "- Mostly build as wrappers around time-proven fortran libraries (fftpack, lapack, fitpack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Physical constants](#physical_constants)\n",
    "2. [Fitting](#fitting)\n",
    "    * [General least-squares fitting using `curve_fit`](#curve_fit)\n",
    "        - [Providing uncertainties and initial guesses](#uncertainties_guesses)\n",
    "        - [Plotting the correlation matrix](#plot_corr_matrix)\n",
    "    * [Unbinned likelihood fits using `minimize`](#minimize)\n",
    "        - [A more complicated example](#minimize_complex)\n",
    "    * [Fitting data with x and y errors using `scipy.odr`](#odr)\n",
    "3. [Fast Fourier Transforms (FFTs)](#fft)\n",
    "4. [Signal filtering](#filtering)\n",
    "5. [Integration](#integration)\n",
    "    * [Function integration](#function_integration)\n",
    "    * [Sample integration](#sample_integration)\n",
    "6. [Interpolation](#interpolation)\n",
    "    * [Linear interpolation](#linear_interpolation)\n",
    "    * [Cubic spline interpolation](#spline_interpolation)\n",
    "8. [Special Functions](#special_functions)\n",
    "    * [Bessel functions](#bessel)\n",
    "    * [Error function and Gaussian CDF](#erf)\n",
    "    * [Orthogonal Polynomials](#ortho_polys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup (run me first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "\n",
    "# we will need to plot stuff later\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['lines.linewidth'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=physical_constants></a>\n",
    "# Physical constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.constants as const\n",
    "const.epsilon_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert temperatures:\n",
    "const.convert_temperature(100, old_scale='C', new_scale='K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# more constants (including units and errors)!\n",
    "\n",
    "for k, v in const.physical_constants.items():\n",
    "    print(k, ':', v)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val, unit, uncertainty = const.physical_constants['muon mass energy equivalent in MeV']\n",
    "\n",
    "val, unit, uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=fitting></a>\n",
    "# Fitting\n",
    "\n",
    "<a id=curve_fit></a>\n",
    "## General least-squares fitting using `curve_fit`\n",
    "\n",
    "Non-linear least-squares with Levenberg-Marquardt numerical minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = -1\n",
    "b = 5\n",
    "\n",
    "x = np.linspace(0, 5, 100)\n",
    "y = np.exp(a * x) + b + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "plt.plot(x, y, '.', label='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "\n",
    "def f(x, a, b):\n",
    "    return np.exp(a * x) + b\n",
    "\n",
    "\n",
    "params, covariance_matrix = curve_fit(f, x, y)\n",
    "\n",
    "uncertainties = np.sqrt(np.diag(covariance_matrix))\n",
    "\n",
    "print('a = {:5.2f} ± {:.2f}'.format(params[0], uncertainties[0]))\n",
    "print('b = {:5.2f} ± {:.2f}'.format(params[1], uncertainties[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_plot = np.linspace(-0.1, 5.1, 1000)\n",
    "\n",
    "plt.plot(x, y, '.', label='data')\n",
    "plt.plot(x_plot, f(x_plot, *params), label='fit result')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=uncertainties_guesses></a>\n",
    "### Providing uncertainties and initial guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "y = np.sin(5 * np.pi * x + np.pi / 2) \n",
    "yerr = np.full_like(y, 0.2)\n",
    "\n",
    "noise = np.random.normal(0, yerr, 100)\n",
    "\n",
    "y += noise\n",
    "\n",
    "def f(x, a, b):\n",
    "    return np.sin(a * x + b)\n",
    "\n",
    "\n",
    "#params, covariance_matrix = curve_fit(f, x, y)\n",
    "\n",
    "# params, covariance_matrix = curve_fit(\n",
    "#    f, x, y,\n",
    "#    p0=[15, 2],\n",
    "#)\n",
    "\n",
    "params, covariance_matrix = curve_fit(\n",
    "    f, x, y,\n",
    "    p0=[15, 1.5],\n",
    "    sigma=yerr,\n",
    "    absolute_sigma=True,\n",
    ")\n",
    "\n",
    "\n",
    "# plot the stuff\n",
    "\n",
    "x_plot = np.linspace(-0.1, 1.1, 1000)\n",
    "\n",
    "plt.plot(x, y, '.', label='data')\n",
    "plt.plot(x_plot, f(x_plot, *params), label='fit result')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=plot_corr_matrix></a>\n",
    "### Plotting the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cov2cor(cov):\n",
    "    '''Convert the covariance matrix to the correlation matrix'''\n",
    "    D = np.diag(1 / np.sqrt(np.diag(cov)))\n",
    "    return D @ cov @ D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix = cov2cor(covariance_matrix)\n",
    "\n",
    "plt.matshow(correlation_matrix, vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "plt.colorbar(shrink=0.8);\n",
    "\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=minimize></a>\n",
    "## Unbinned likelihood fits using `minimize`\n",
    "\n",
    "Simple example: an unbinned negative log-likelihood fit for a poissonian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_ = 15\n",
    "k = np.random.poisson(lambda_, 100)\n",
    "\n",
    "# make sure to use bins of integer width, centered around the integer\n",
    "bin_edges = np.arange(0, 31) - 0.5\n",
    "\n",
    "plt.hist(k, bins=bin_edges);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poisson pdf:\n",
    "\n",
    "$$ \n",
    "f(k, \\lambda) = \\frac{\\lambda^k}{k!} \\mathrm{e}^{-\\lambda}\n",
    "$$\n",
    "\n",
    "So the likelihood is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\prod_{i=0}^{N} \\frac{\\lambda^{k_i}}{k_i!} \\mathrm{e}^{-\\lambda}\n",
    "$$\n",
    "\n",
    "It's often easier to minimize $-\\log(\\mathcal{L})$, let's see:\n",
    "\n",
    "$$\n",
    "-\\log(\\mathcal{L}) = - \\sum_{i=0}^{N}\\bigl( k_i \\log(\\lambda) - \\log{k_i!} - \\lambda \\bigr)\n",
    "$$\n",
    "\n",
    "We are interested in the minimum reletive to $\\lambda$, so we dismiss constant term concerning $\\lambda$ \n",
    "$$\n",
    "-\\log(\\mathcal{L}) = \\sum_{i=0}^{N}\\bigl( \\lambda - k_i \\log(\\lambda) \\bigr)   \n",
    "$$\n",
    "\n",
    "This looks indeed easier to minimize than the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def negative_log_likelihood(lambda_, k):\n",
    "    return np.sum(lambda_ - k * np.log(lambda_))\n",
    "\n",
    "result = minimize(\n",
    "    negative_log_likelihood,\n",
    "    x0=(10, ),   # initial guess\n",
    "    args=(k, ),  # additional arguments for the function to minimize\n",
    ")\n",
    "\n",
    "result\n",
    "\n",
    "print('True λ = {}'.format(lambda_))\n",
    "print('Fit: λ = {:.2f} ± {:.2f}'.format(result.x[0], np.sqrt(result.hess_inv[0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* minimize has lots of options for different minimization algorithms\n",
    "* Also able to respect bounds and constraints (with certain algorithms)\n",
    "* It is worth to write down you problems and simplify the (log)Likelihood as much as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=minimize_complex></a>\n",
    "### A more complicated example\n",
    "\n",
    "Fitting a gaussian with an exponential background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have two distributions, an exponential and a gaussian:\n",
    "\n",
    "$$\n",
    "f(x, \\mu, \\sigma, \\tau, p) =  p \\cdot \\frac{1}{\\sqrt{2 \\pi}} \\mathrm{e}^{-0.5 \\frac{(x - \\mu)^2}{\\sigma^2}} + (1 - p) \\cdot \\frac{1}{\\tau} \\mathrm{e}^{- x / \\tau}\n",
    "$$\n",
    "\n",
    "Likelihood:\n",
    "$$\n",
    "\\mathcal{L} = \\prod_{i = 0}^N \\bigl( p \\cdot \\frac{1}{\\sqrt{2 \\pi}} \\mathrm{e}^{-0.5 \\frac{(x_i - \\mu)^2}{\\sigma^2}} + (1 - p) \\cdot \\frac{1}{\\tau} \\mathrm{e}^{- x_i / \\tau} \\bigr)\n",
    "$$\n",
    "Negative log-likelihood:\n",
    "\n",
    "$$\n",
    "-\\log(\\mathcal{L}) = -\\sum_{i = 0}^N \\log\\bigl( p \\cdot \\frac{1}{\\sqrt{2 \\pi}} \\mathrm{e}^{-0.5 \\frac{(x_i - \\mu)^2}{\\sigma^2}} + (1 - p) \\cdot \\frac{1}{\\tau} \\mathrm{e}^{- x_i / \\tau} \\bigr)\n",
    "$$\n",
    "\n",
    "But we can make use of the built in scipy distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm, expon\n",
    "\n",
    "x = np.append(\n",
    "    norm.rvs(loc=15, scale=2, size=500), \n",
    "    expon.rvs(scale=10, size=4500),\n",
    ")\n",
    "\n",
    "def pdf(x, mu, sigma, tau, p):\n",
    "    return p*norm.pdf(x, mu, sigma) + (1 - p)*expon.pdf(x, scale=tau)\n",
    "\n",
    "\n",
    "def negative_log_likelihood(params, x):\n",
    "    mu, sigma, tau, p = params\n",
    "    neg_l = -np.sum(np.log(pdf(x, mu, sigma, tau, p)))\n",
    "    return neg_l\n",
    "\n",
    "\n",
    "result = minimize(\n",
    "    negative_log_likelihood,\n",
    "    x0=(12, 1.5, 8, 0.2),   # initial guess\n",
    "    args=(x, ),            # additional arguments for the function to minimize\n",
    "    bounds=[\n",
    "        (None, None),      # no bounds for mu\n",
    "        (1e-32, None),         # sigma > 0\n",
    "        (1e-32, None),         # tau > 0\n",
    "        (0, 1),            # 0 <= p <= 1\n",
    "    ],\n",
    "    method='L-BFGS-B', # method that supports bounds\n",
    ")\n",
    "\n",
    "x_plot = np.linspace(0, 100, 1000)\n",
    "\n",
    "plt.hist(x, bins=100, normed=True)\n",
    "plt.plot(x_plot, pdf(x_plot, *result.x))\n",
    "\n",
    "print(result.hess_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the covariance matrix as normal numpy array\n",
    "covariance_matrix = result.hess_inv.todense()\n",
    "\n",
    "\n",
    "correlation_matrix = cov2cor(covariance_matrix)\n",
    "\n",
    "plt.matshow(correlation_matrix, vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "\n",
    "plt.colorbar(shrink=0.8);\n",
    "plt.xticks(np.arange(4), ['μ', 'σ', 'τ', 'p'])\n",
    "plt.yticks(np.arange(4), ['μ', 'σ', 'τ', 'p'])\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=odr></a>\n",
    "## Fitting data with x and y errors using `scipy.odr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most fitting routines only handle uncertainties on dependent variable (y-coordinate), and in most cases this is fine.\n",
    "However, sometimes you may also want to consider errors on the independent variable (x-coordinate). This generally occurs\n",
    "when you have some (non-negligible) uncertainty associated with your measurement apparatus.\n",
    "\n",
    "Consider the following random data set with both x and y uncertainties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate some data\n",
    "real_values = np.array([1.5, -3])\n",
    "x = np.linspace(0, 1, 10)\n",
    "y = real_values[0]*x + real_values[1]\n",
    "xerr = np.full_like(x, 0.1)\n",
    "yerr = np.full_like(y, 0.05)\n",
    "\n",
    "# add noise to the data\n",
    "x += np.random.normal(0, xerr, 10)\n",
    "y += np.random.normal(0, yerr, 10)\n",
    "\n",
    "# plot the data\n",
    "plt.errorbar(x, y, xerr=xerr, yerr=yerr, fmt='o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cases like these can be handled using orthogonal distance regression (ODR). When the independent variable is error-free (or the errors are negligibly small), the residuals are computed as the *vertical* distance between the data points and the fit. This is the ordinary least-squares method.\n",
    "\n",
    "In the specific case that the x and y uncertainties are equal, which would occur if the same measurement device is used to measure both the independent and dependent variables, the residual to be minimized will actually be *perpendicular* (orthogonal) to the fit curve. Note that Python's ODR fit routines do not require that the x and y uncertainties are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy.odr as odr\n",
    "\n",
    "# function we want to fit (in this case, a line)\n",
    "def f(B, x):\n",
    "    return B[0]*x + B[1]\n",
    "\n",
    "# do the fit!\n",
    "guess = [5, 0]\n",
    "linear = odr.Model(f)\n",
    "data = odr.RealData(x, y, sx=xerr, sy=yerr)\n",
    "odr_fit = odr.ODR(data, linear, beta0=guess)\n",
    "odr_output = odr_fit.run()\n",
    "odr_output.pprint()   # pprint = 'pretty print' function\n",
    "\n",
    "# plot data and ODR fit\n",
    "z = np.linspace(-0.1, 1.1, 100)\n",
    "plt.errorbar(x, y, xerr=xerr, yerr=yerr, fmt='o')\n",
    "plt.plot(z, f(odr_output.beta, z), 'k--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we do a comparison to a fit with ordinary least-squares (curve_fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def g(x, m, b):\n",
    "    return m*x + b\n",
    "\n",
    "params, covariance_matrix = curve_fit(g, x, y, sigma=yerr, p0=guess)\n",
    "\n",
    "plt.errorbar(x, y, xerr=xerr, yerr=yerr, fmt='o')\n",
    "plt.plot(z, f(odr_output.beta, z), 'k--', label='ODR Fit')\n",
    "plt.plot(z, g(z, *params), 'g-.', label='curve_fit')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "print('ODR Fit Results:  ', odr_output.beta)\n",
    "print('curve_fit Results:', params)\n",
    "print('Real Values:', real_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the x-uncertainties are relatively small, in general curve_fit will produce a better result. However, if the uncertainties on the independent variable are large and/or there is a rapidly changing region of your curve where the x-errors are important, ODR fitting may produce a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=fft></a>\n",
    "# Fast Fourier Transforms (FFTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq1 = 5\n",
    "freq2 = 50\n",
    "\n",
    "t = np.linspace(0, 1, 1024*10)\n",
    "y = np.sin(2*np.pi*freq1*t) + np.sin(2*np.pi*freq2*t)\n",
    "\n",
    "# add some white noise\n",
    "y += np.random.normal(y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(t, y, s=10, alpha=0.25, lw=0)\n",
    "plt.xlabel(r'$t \\ /\\ \\mathrm{s}$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import fftpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = fftpack.rfft(y)\n",
    "f = fftpack.rfftfreq(len(t), t[1] - t[0])\n",
    "\n",
    "plt.axvline(freq1, color='lightgray', lw=5)\n",
    "plt.axvline(freq2, color='lightgray', lw=5)\n",
    "\n",
    "plt.plot(f, np.abs(z)**2)\n",
    "\n",
    "plt.xlabel('f / Hz')\n",
    "plt.xscale('log')\n",
    "# plt.yscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=filtering></a>\n",
    "# Signal filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this noisy data set with outliers. The data is a so-called S-curve, and we want to identify the midpoint of the falling edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import ndtr\n",
    "\n",
    "def s_curve(x, a, b):\n",
    "    return ndtr(-a*(x - b))\n",
    "\n",
    "# generate mildly noisy data using Gaussian CDF (see end of this notebook)\n",
    "real_params = [2.5, 3]\n",
    "x = np.linspace(0, 5, 20)\n",
    "y = s_curve(x, *real_params)\n",
    "y += np.random.normal(0, 0.025, len(y))\n",
    "\n",
    "# add 4 bad data points\n",
    "outlier_xcoords = [2, 6, 10, 15]\n",
    "y[outlier_xcoords] = np.random.uniform(0.2, 2, size=4)\n",
    "plt.plot(x, y, 'bo')\n",
    "\n",
    "# attempt to fit\n",
    "params, __ = curve_fit(s_curve, x, y)\n",
    "z = np.linspace(0, 5, 100)\n",
    "plt.plot(z, s_curve(z, *params), 'k--')\n",
    "print('Real value:', real_params[1])\n",
    "print('Fit value:', params[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see clearly in the data that the mid-point of the S-curve is at about x=3 (which is the real value), but the outliers destroy the fit. We can remove them easily with a median filter. A median filter is particularly suited to edge detection cases, since it tends to preserve edges well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt\n",
    "\n",
    "filtered_y = medfilt(y)\n",
    "\n",
    "params, __ = curve_fit(s_curve, x, filtered_y)\n",
    "print('Real value:', real_params[1])\n",
    "print('Fit value:', params[1])\n",
    "\n",
    "z = np.linspace(0, 5, 100)\n",
    "plt.plot(x, y, 'k*', label='Before Filtering')\n",
    "plt.plot(x, filtered_y, 'bo', label='After Filtering')\n",
    "plt.plot(z, s_curve(z, *params), 'g--')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example implementation of a low-pass [Butterworth filter](https://en.wikipedia.org/wiki/Butterworth_filter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "def lowpass_filter(data, cutoff, fs, order=5):\n",
    "    \"\"\"\n",
    "    Digital Butterworth low-pass filter.\n",
    "    \n",
    "    data   : 1D array of data to be filtered\n",
    "    cutoff : cutoff frequency in Hz\n",
    "    fs     : sampling frequency (samples/second)\n",
    "    \"\"\"\n",
    "    nyquist_frequency = fs/2\n",
    "    normal_cutoff = cutoff/nyquist_frequency\n",
    "    b, a = butter(order, normal_cutoff, btype='low')\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are the unfortunate recipient of the following noisy data, which contains noise at two different (unknown) frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('../resources/scipy_filter_data.dat')\n",
    "t = data[:, 0]\n",
    "y = data[:, 1]\n",
    "sample_freq = (len(t) - 1)/(t[-1])\n",
    "plt.plot(t, y);   # these are your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhere in this mess is a Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def gaussian(x, mu, sigma, A):\n",
    "    return A * norm.pdf(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a FFT to identify the two offending noise frequencies. Then convert the `lowpass_filter` above into a bandstop filter (hint: it is a trivial modification), and remove the offending noise from the data as much as possible (it won't be perfect). Finally, use `curvefit` to fit a Gaussian to the data, thereby recovering the original signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -r 3-52 solutions/07_01_scipy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=integration></a>\n",
    "# Integration\n",
    "\n",
    "Scipy integration routines are discussed in the [Scipy documentation](https://docs.scipy.org/doc/scipy/reference/tutorial/integrate.html). We will look at the two most common routines here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=function_integration></a>\n",
    "## Function integration\n",
    "\n",
    "`quad` is used to evaluate definite 1D numerical integrals. For example, assume we want to integrate a quadratic polynomial $f(x) = 3x^2 + 6x - 9$ over an interval $x \\in [0, 5]$. Analytically, the answer is:\n",
    "\n",
    "$$ \\int_0^5 3x^2 + 6x - 9 \\ dx = \\left[ x^3 + 3x^2 - 9x \\right]_{x = 0}^{x = 5} = 155 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "\n",
    "def f(x):\n",
    "    return 3*x**2 + 6*x - 9\n",
    "\n",
    "quad(f, 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter `quad` returns is the answer; the second is an estimate of the absolute error in the result.\n",
    "\n",
    "For 2D, 3D, or n-dimensional integrals , use `dblquad`, `tplquad`, or `nquad`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some more interesting functions, Scipy's other function integration routines might be helpful:\n",
    "* `quadrature` : [Gaussian quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature)\n",
    "* `romberg` : [Romberg integration](https://en.wikipedia.org/wiki/Romberg%27s_method)\n",
    "\n",
    "For example, consider the $\\mathrm{sinc}$ function:\n",
    "\n",
    "$$\n",
    "\\mathrm{sinc}(x) \\equiv\n",
    "\\begin{cases} \n",
    "1 & x = 0 \\\\\n",
    "\\sin(x)/x & \\mathrm{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sinc(x):\n",
    "    return np.sin(x) / x\n",
    "\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = sinc(x)\n",
    "plt.plot(x, y)\n",
    "plt.title('Sinc Function')\n",
    "\n",
    "print(quad(sinc, -10, 10))   # fails\n",
    "\n",
    "# numpys sinc handles the singularity correctly\n",
    "print(quad(np.sinc, -10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`quad` struggles with $\\mathrm{sinc}$, but it can be easily handled with Gaussian quadrature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import quadrature\n",
    "\n",
    "# quadrature may complain, but it will work in the end\n",
    "print(quadrature(sinc, -10, 10)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result agrees with Mathematica to 13 decimal places (even though only 11 are shown). Note that the problem is the singularity at $x=0$; if we change the boundaries to, say, [-10.1, 10], then it works fine. Also, writing our sinc function more cleverly would eliminate the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=sampleintegration></a>\n",
    "## Sample integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a collection of points that you want to integrate, you could use an [interpolation function](#interpolation) and pass it to `quad`. A better alternative is to use the purpose-built functions `trapz`, `romb`, and `simps`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the $\\mathrm{sinc}$ function again as an example. The most naive (and surprisingly robust) integration method is using the trapazoid rule, which is implemented in `trapz`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import trapz\n",
    "\n",
    "# 50 grid points\n",
    "x = np.linspace(-10, 10)\n",
    "y = sinc(x)\n",
    "print('  50 points:', trapz(y, x))   # note the order of the arguments: y, x\n",
    "\n",
    "# 1000 grid points\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = sinc(x)\n",
    "print('1000 points:', trapz(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=interpolation></a>\n",
    "# Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=linear_interpolation></a>\n",
    "## Linear interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you would like to interpolate between two points $(x_0, y_0)$ and $(x_1, y_1)$. You could do this by hand:\n",
    "\n",
    "$$y(x) = y_0 + (x - x_0) \\frac{y_1 - y_0}{x_1 - x_0}$$\n",
    "\n",
    "Simple enough, but it is annoying to look up or derive the formula. Also, what if you want values less than $x_0$ to stay at the value of $y_0$, and likewise for values greater than $x_1$? Then you need to add `if` statements, and check the logic, etc. Too much work.\n",
    "\n",
    "Instead, there is a simple function for almost all of your interpolation needs: `interp1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "x = (1, 2)\n",
    "y = (5, 7)\n",
    "print('Points:', list(zip(x, y)))\n",
    "\n",
    "f = interp1d(x, y)\n",
    "z = [1.25, 1.5, 1.75]\n",
    "print('Interpolation:', list(zip(z, f(z))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, if you try to use an x-coordinate outside of the interval $[x_0, x_1]$, a `ValueError` will be raised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f(2.5)   # uncomment to run me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we haven't told `interp1d` how we want to handle the boundaries. This is done using the `fill_value` keyword argument. There are a few options:\n",
    "\n",
    "1. Set values outside of the interval $[x_0, x_1]$ to a float.\n",
    "2. Set values $< x_0$ to `below` and values $> x_1$ to `above` by passing a tuple, `(below, above)`.\n",
    "3. Extrapolate points outside the interval by passing `extrapolate`.\n",
    "\n",
    "We also need to tell `interp1d` not to raise a `ValueError` by setting the `bounds_error` keyword to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = [0.5, 1, 1.5, 2, 2.5]\n",
    "\n",
    "f = interp1d(x, y, bounds_error=False, fill_value=0)\n",
    "print(\"Option 1:\", list(zip(z, f(z))))\n",
    "\n",
    "f = interp1d(x, y, bounds_error=False, fill_value=y)   # fill with endpoint values\n",
    "print(\"Option 2:\", list(zip(z, f(z))))\n",
    "\n",
    "f = interp1d(x, y, fill_value='extrapolate')   # bounds_error set to False automatically\n",
    "print(\"Option 3:\", list(zip(z, f(z))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=spline_interpolation></a>\n",
    "## Cubic spline interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cubic splines](http://mathworld.wolfram.com/CubicSpline.html) are what are most commonly used when you want to interpolate between points *smoothly*.\n",
    "\n",
    "Cubic spline interpolation is so common, it has its own method, `CubicSpline`, which produces generally better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "npoints = 5\n",
    "x = np.arange(npoints)\n",
    "y = np.random.random(npoints)\n",
    "plt.plot(x, y, label='linear')\n",
    "\n",
    "f = interp1d(x, y, kind='cubic')\n",
    "z = np.linspace(np.min(x), np.max(x), 100)\n",
    "plt.plot(z, f(z), label='interp1d cubic')\n",
    "\n",
    "f = CubicSpline(x, y)\n",
    "z = np.linspace(np.min(x), np.max(x), 100)\n",
    "plt.plot(z, f(z), label='CubicSpline')\n",
    "\n",
    "plt.plot(x, y, 'ko')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=special_functions></a>\n",
    "# Special Functions\n",
    "\n",
    "A complete list of scipy special functions can be found [here](https://docs.scipy.org/doc/scipy-0.14.0/reference/special.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=bessel></a>\n",
    "## Bessel functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import jn\n",
    "\n",
    "x = np.linspace(0, 10, 100)\n",
    "for n in range(6):\n",
    "    plt.plot(x, jn(n, x), label=r'$\\mathtt{J}_{%i}(x)$' % n)\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mpl_toolkits.mplot3d.axes3d as plt3d\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def airy_disk(x):\n",
    "    mask = x != 0\n",
    "    result = np.empty_like(x)\n",
    "    result[~mask] = 1.0\n",
    "    result[mask] = (2 * jn(1, x[mask]) / x[mask])**2\n",
    "    return result\n",
    "\n",
    "# 2D plot\n",
    "r = np.linspace(-10, 10, 500)\n",
    "plt.plot(r, airy_disk(r))\n",
    "\n",
    "# 3D plot\n",
    "x = np.arange(-10, 10.1, 0.1)\n",
    "y = np.arange(-10, 10.1, 0.1)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = airy_disk(np.sqrt(X**2 + Y**2))\n",
    "\n",
    "result\n",
    "fig = plt.figure()\n",
    "ax = plt3d.Axes3D(fig)\n",
    "ax.plot_surface(X, Y, Z, cmap='gray', norm=LogNorm(), lw=0)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=erf></a>\n",
    "## Error function and Gaussian CDF\n",
    "\n",
    "CDF = cumulative distribution function\n",
    "\n",
    "$$\\mathrm{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z \\exp\\left( -t^2 \\right) dt $$\n",
    "$$\\mathrm{ndtr}(z) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^z \\exp\\left( \\frac{-t^2}{2} \\right) dt $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import erf, ndtr\n",
    "\n",
    "def gaussian(z):\n",
    "    return np.exp(-z**2)\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "plt.plot(x, gaussian(x), label='Gaussian')\n",
    "plt.plot(x, erf(x), label='Error Function')\n",
    "plt.plot(x, ndtr(x), label='Gaussian CDF')\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=ortho_polys></a>\n",
    "## Orthogonal Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import eval_legendre, eval_laguerre, eval_hermite, eval_chebyt\n",
    "\n",
    "ortho_poly_dict = {'Legendre': eval_legendre,\n",
    "                   'Laguerre': eval_laguerre,\n",
    "                   'Hermite': eval_hermite,\n",
    "                   'Chebyshev T': eval_chebyt}\n",
    "\n",
    "def plot_ortho_poly(name):\n",
    "    plt.figure()\n",
    "    f = ortho_poly_dict[name]\n",
    "    x = np.linspace(-1, 1, 100)\n",
    "    for n in range(5):\n",
    "        plt.plot(x, f(n, x), label='n = %i' % n)\n",
    "    if name is 'Legendre' or 'Chebyshev' in name:\n",
    "        plt.ylim(-1.1, 1.1)\n",
    "    plt.legend(loc='best', fontsize=16)\n",
    "    plt.title(name + ' Polynomials')\n",
    "    \n",
    "plot_ortho_poly('Legendre')\n",
    "# plot_ortho_poly('Laguerre')\n",
    "# plot_ortho_poly('Hermite')\n",
    "# plot_ortho_poly('Chebyshev T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orthogonal polynomials can be used to construct a series expansion of an arbitrary function, just like $\\sin$ and $\\cos$ are used to construct a Fourier series. For example, we can express a function $f(x)$ as a series of Legendre polynomials $P_n(x)$:\n",
    "\n",
    "$$ f(x) = \\sum_{n=0}^{\\infty} a_n P_n(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Legendre polynomials are orthogonal on the interval $x \\in [-1, 1]$, where they obey the following orthogonality relationship:\n",
    "$$ \\int_{-1}^{1} P_n(x) \\, P_m(x) \\, dx = \\frac{2}{2 m + 1} \\delta_{mn} $$\n",
    "\n",
    "With $f(x) = sin(\\pi x)$, write a function to calculate the coefficients $a_n$ of the Legendre series. Then plot $f(x)$ and the Legendre series for $x \\in [-1, 1]$. Calculate as many coefficients as are needed for the series to essentially the same as $f(x)$ (it will be less than ten).\n",
    "\n",
    "If you are struggling with the math, look [here](http://mathworld.wolfram.com/Fourier-LegendreSeries.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -r 57-80 solutions/07_01_scipy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information\n",
    "\n",
    "#### Fitting with (i)minuit\n",
    "\n",
    "One of the most extended minimization packages classically used for fitting is [MINUIT](https://en.wikipedia.org/wiki/MINUIT). Python allows to use this software via wrapping the C++ code into a python package named [iminuit](https://github.com/iminuit/iminuit).\n",
    "\n",
    "For more information see [iminuit documentation](http://iminuit.readthedocs.io/en/latest/), and the [available tutorials](https://github.com/iminuit/iminuit/blob/master/tutorial/tutorial.py) within the GitHub project.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
