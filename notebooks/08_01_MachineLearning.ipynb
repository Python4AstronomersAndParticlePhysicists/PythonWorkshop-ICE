{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> # In God we trust, all others bring data.\n",
    "\n",
    ">William Edwards Deming (1900-1993)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn (sklearn)\n",
    "\n",
    "Scikit-learn is a Python library containing hundreds of methods for machine learning purposes. \n",
    "It is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n",
    "\n",
    "It leverages NumPy and c-extensions for performance and provides many out-of-the-box tools for performing data mining tasks.\n",
    "\n",
    "It's a free and open source library and is used in many scientific publications.\n",
    "\n",
    "![sklearn logo](./ml/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A word on what machines can (and cannot) do\n",
    "\n",
    "The term *Artifical Inteligence* was coined by John McArthy for the 1956 Dartmouth Conference. \n",
    "\n",
    "The idea of a *mechanical brain* is much older. \n",
    "\n",
    "![The Turk](https://upload.wikimedia.org/wikipedia/commons/8/8b/Tuerkischer_schachspieler_windisch4.jpg)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Church-Turing Thesis\n",
    "\n",
    "> Every effectively calculable function is a computable function.\n",
    "\n",
    "Using Turings definitions of computability\n",
    "\n",
    "> \"We shall use the expression 'computable function' to mean a function calculable by a machine, and let 'effectively calculable' refer to the intuitive idea without particular identification with any one of these definitions.\"\n",
    "\n",
    "To put it into somewhat more understandable words:\n",
    "\n",
    "* If something is intuitivly calculable (in whatever manner you can think of), it can be computed by a machine. \n",
    "\n",
    "* If something can be computed by a machine, it can be computed by a brain. (given enough time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Are computers just as good as human brains then?__\n",
    "\n",
    "They seem to work well together.\n",
    "\n",
    "Computer assisted proof of the Four-Color-Problem in 1989:\n",
    "    \n",
    ">  ...part of the proof was verified in over 400 pages of microfiche, which had to be checked by hand with the assistance of Haken's daughter...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"bottom\">\n",
    "  <div style=\"float: left; width: 40%;\">\n",
    "  \n",
    "  __Well, no.__ (not yet at least)\n",
    "  \n",
    "  <br>\n",
    "  Easy problem for machines:\n",
    "    <ul>\n",
    "    <li>  Is this text written in the english language?</li>\n",
    "    </ul>\n",
    "  Impossible problem for machines:\n",
    "    <ul>\n",
    "    <li> What does this text mean?</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"float: left; width: 60%;\">\n",
    "      <img width=\"100%\" src=\"./ml/images/alice.jpg\"> </img>\n",
    "  </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Things to keep in mind while talking about artifical intelligence\n",
    "\n",
    "* Logical reasoning (within some limitations) is possible for machines as long as all relations and symbols are strictly defined \n",
    "\n",
    "* Mapping syntax and symbols to semantics and objects in the real world is something inherently human (or intelligent).\n",
    "\n",
    "* There is not really a powerfull formal equivalent for human creativity.\n",
    "\n",
    "* __Machines cannot \"learn\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br><br><br><br><br><br><br><br><br><br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning (Supervised)\n",
    "\n",
    "## Mathematical Notation and Problem Description\n",
    "\n",
    "*For more details see \"Elements of statistical Learning\" by Trevor Hastie. (Available for free as an E-Book)* \n",
    "\n",
    "I'll try to follow some naming conventions along this notebook. They are the same as in the book (for the most parts).\n",
    "\n",
    "* Uppercase letters such as $X$ or $Y$ denote generic aspects of a variable (i.e. the actual random variable)\n",
    "* Observed values are written in lowercase. The ith observed value of $X$ is written as $x_i$\n",
    "* Matrices are written in bold uppercase letters as in $\\mathbf{X}$\n",
    "* Observations map as *rows* in the matrix while the observed variables are the *columns*.\n",
    "\n",
    "So if I measure two observables $p = 2$ the size and weight of $N = 100$ people, I get a $N \\times p$ matrix $\\mathbf{X}$.\n",
    "One observation in that matrix is denoted as $x_i = [ size, weight ]$ while all observations of the variable size are denoted by $\\mathbf{x}_j$ \n",
    "\n",
    "Heres one possible definition of supervised machine learning:\n",
    "\n",
    "> Given a $N \\times p$ matrix $\\mathbf{X}$ matrix and some associated output vector $\\mathbf{Y} \\in \\mathbb{R}^N$,\n",
    " find a function $f(X) = \\hat{Y}$ that takes a vector $X \\in \\mathbb{R}^p$ and returns a prediction for $\\hat{Y}$\n",
    " where some \"loss function\" $L(Y, f(X))$ is minimized for all $X$.\n",
    " \n",
    "We now look at an example to see what that actually entails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### The Titanic Example. Learning from disaster.\n",
    "\n",
    "In the spring of 1912 the R.M.S. Titanic embarked on a journey to cross the Atlantic ocean. Unfortunately it hit an iceberg on the night of April 14th and sank shortly afterwards.\n",
    "\n",
    "The disaster caused widespread outrage over waht was seen as lax safety regulations and reckles behavoiur by some. New maritime safety laws were put in place after the sinking that are still in place today.\n",
    "\n",
    "What can _we_ learn from the Titanic just by looking at its passenger data?\n",
    "\n",
    "Our data contains a list of name, gender, age and ticket price for each (known) passenger.  \n",
    "\n",
    "![NYT headline about the Titanic](./ml/images/nyt_titanic.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ml import plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def set_rc_params():\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    plt.rcParams['lines.linewidth'] = 2\n",
    "    plt.rcParams['xtick.labelsize'] = 13\n",
    "    plt.rcParams['ytick.labelsize'] = 13\n",
    "    plt.rcParams['axes.labelsize'] = 14\n",
    "    plt.rcParams['axes.titlesize'] = 14\n",
    "    plt.rcParams['legend.fontsize'] = 13\n",
    "\n",
    "def set_sns():\n",
    "    sns.set(context='notebook')\n",
    "    set_rc_params()\n",
    "\n",
    "    \n",
    "def set_mpl():\n",
    "    sns.reset_orig()\n",
    "    set_rc_params()\n",
    "\n",
    "\n",
    "set_sns()\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "def read_titanic():\n",
    "    data = pd.read_csv('../resources/titanic_train.csv', index_col='PassengerId').dropna(subset=['Age'])\n",
    "    data['Survived_Code'] = data.Survived\n",
    "    data['Pclass_Code'] = data.Pclass\n",
    "    data.Survived = pd.Categorical.from_codes(data.Survived, categories=['no', 'yes'])\n",
    "    data.Pclass = pd.Categorical.from_codes(data.Pclass - 1, categories=['1st', '2nd', '3rd'])\n",
    "    data.Sex = pd.Categorical(data.Sex)\n",
    "    data['Sex_Code'] = data.Sex.cat.codes\n",
    "    return data\n",
    "\n",
    "data = read_titanic()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data.Survived.value_counts().plot.pie(autopct='%.2f %%')\n",
    "plt.axes().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__The task:__\n",
    "\n",
    "Given a vector $X = (Name, Class, Age, Sex)$ can we find a function $f_{survival}(x) \\in \\{{yes, no}\\}$ that accurately predicts the survival of the passengers in most cases?\n",
    "\n",
    "How do we know if that function $f_{survival}(x)$  is any good?\n",
    "\n",
    "To get some sense of the quality of this predictor we gather the following numbers.\n",
    "\n",
    "* __True Positives__ $TP$, The number of correctly predicted events that belong to the 'positive' class\n",
    "* __False Positives__ $FP$, The number of events falsely predicted as positive that actually belong to the 'negative' class\n",
    "* __True Negatives__ $TN$, The number of correctly predicted events that belong to the 'negative' class\n",
    "* __False Negatives__ $FN$, The number of events falsely predicted as negative that actually belong to the 'positive' class\n",
    "\n",
    "\n",
    "We can look at the fraction of correctly labeled observations in the data\n",
    "\n",
    "$$\n",
    "    accuracy(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{N} \\sum_{i = 1}^N \\mathbb{1}(y_i = \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "or simply put \n",
    "\n",
    "$$\n",
    "    accuracy(\\mathbf{y}, \\mathbf{\\hat{y}}) =  \\frac{TP + TN}{ TP + FP + FN + TN} = \\frac{\\text{correclty predicted}}{\\text{total number of observations}}.\n",
    "$$\n",
    "\n",
    "\n",
    "Now we try to find a function where the accuracy is higher than 0.5\n",
    "\n",
    "\n",
    "__One possible solution__:\n",
    "\n",
    "Let's presume rich people get to go into lifeboats.\n",
    "\n",
    "```\n",
    "def f_class(passenger):\n",
    "    if passenger.Pclass == 1:\n",
    "        return 'yes'\n",
    "    else:\n",
    "        return 'no'\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def f_class(passenger_class):\n",
    "    return 'yes' if passenger_class == '1st' else 'no'\n",
    "\n",
    "data = read_titanic()\n",
    "prediction = data['Pclass'].apply(f_class)\n",
    "truth = data['Survived']\n",
    "\n",
    "plots.plot_bars_and_confusion(truth=truth, prediction=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What about the women? Maybe we get a better predictor.\n",
    " \n",
    "```\n",
    "def f_survival(passenger):\n",
    "    if passenger.Sex == 'female':\n",
    "        return 'yes'\n",
    "    else:\n",
    "        return 'no'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def f_sex(passenger_sex):\n",
    "    return 'yes' if passenger_sex == 'female' else 'no'\n",
    "\n",
    "data = read_titanic()\n",
    "truth = data['Survived']\n",
    "prediction = data['Sex'].apply(f_sex)\n",
    "\n",
    "plots.plot_bars_and_confusion(truth=truth, prediction=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we can do better by using some combination of variables for our prediction. But how do you find a good combination of variables?\n",
    "\n",
    "We could use visualizations to see correlations or obvious structures in the data. \n",
    "\n",
    "We could also try yo learn more about what happened on the Titanic. \n",
    "\n",
    "Perhaps even by watching that movie where Leonardo Di Caprio drowns in the end. \n",
    "\n",
    "![Movie Snaphsot](./ml/images/titanic-movie.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Models\n",
    "\n",
    "Can we improve our predictor by combining more variables into one predictor?\n",
    "\n",
    "Lets presume a linear weighted combination of variables:\n",
    "\n",
    "$$\n",
    "f(X)= \\hat{\\beta}_0 + \\sum_{j=1}^p X_j \\hat{\\beta}_j\n",
    "$$\n",
    "\n",
    "or in our case when combining sex and money:\n",
    "\n",
    "$$\n",
    "f(X)= \\hat{\\beta}_0 + X_{Class} \\hat{\\beta}_{Class} + X_{Sex} \\hat{\\beta}_{Sex}  \n",
    "$$\n",
    "\n",
    "How do you find those weights?\n",
    "\n",
    "Choose and then optimize a loss function. In this case the popular residual sum of squares \n",
    "\n",
    "$$L(\\beta) = RSS(\\mathbf{\\beta}) = \\sum_{i=1}^N (Y_i - X_i^T \\beta)^2 $$\n",
    "\n",
    "Rewrite the problem in matrix form:\n",
    "\n",
    "\\begin{align}\n",
    "X^T &= (1, Class, Sex) \\\\ \n",
    "\\mathbf{\\hat{\\beta}}^T &= (\\hat{\\beta}_0, \\hat{\\beta}_{Class}, \\hat{\\beta}_{Sex}) \\\\\n",
    "\\mathbf{y} &= {Y_1, \\ldots, Y_N} \n",
    "\\end{align}\n",
    "\n",
    "Makes the formulation more compact for the predictor\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = X^T \\hat{\\beta}\n",
    "$$\n",
    "\n",
    "and the loss function\n",
    "\n",
    "$$\n",
    "RSS(\\beta) = (\\mathbf{y} - \\mathbf{X} \\beta)^T (\\mathbf{y} - \\mathbf{X} \\beta )\n",
    "$$\n",
    "\n",
    "Now we optimize the loss function just like we would any other function, by setting the derivative equals to zero.\n",
    "\n",
    "$$\n",
    "{RSS}^\\prime(\\beta) =  \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\beta ) \\stackrel{!}{=} 0\n",
    "$$\n",
    "\n",
    "Solving for $\\beta$ leads to\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "\n",
    "We just performed  __Linear Least Squares__ regression.\n",
    "\n",
    "Now we can define a function to predict passenger survival according to\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\begin{cases}\n",
    "\\text{Yes}, & \\text{if $ f(X) \\gt 0.5$} \\\\\n",
    "\\text{No}, & \\text{if $ f(X) \\le 0.5$}\n",
    "\\end{cases}\n",
    "$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Excersise 1: Linear Regression and Classification (10 - 15 minutes)\n",
    "\n",
    "Create an artificial 2D dataset with two classes and use the least squares method to seperate them.\n",
    "\n",
    "1. Create 2D gaussians of data for classes A and B\n",
    "    \n",
    "        A = np.random.multivariate_normal(mean=[1, 1], cov=[[2,1], [1,2]], size=200)\n",
    "        B = ....\n",
    "    \n",
    "2. Plot the distributions into one scatter plot\n",
    "        \n",
    "        plt.scatter(x_coordinates, y_coordinates, s=size_of_points)\n",
    "        \n",
    "3. Create the $X$ matrix and the output (label) vector $Y$\n",
    "        \n",
    "        X = np.vstack([A, B])\n",
    "        Y = [0, 0, 0, ..., 1, 1, 1, ...]\n",
    "        \n",
    "4. Use scikit-learn's linear regressor to find the parameters for $f(X_1, X_2) = \\hat{Y}$.\n",
    "\n",
    "        from sklearn import linear_model\n",
    "        reg = linear_model.LinearRegression()\n",
    "        reg.fit(X, Y)\n",
    "        b_1, b_2 = reg.coef_\n",
    "        b_0 = reg.intercept_\n",
    "\n",
    "5. Draw a dashed line into the plot where $f(X_1, X_2) = 0.5$.\n",
    "\n",
    "        x1s = np.linspace(-8, 8)\n",
    "        x2s = ...\n",
    "        ...\n",
    "        plt.plot(x1s, x2s, color='gray', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "np.random.seed(1234)\n",
    "# create two gaussians\n",
    "A = np.random.multivariate_normal(mean=[1, 1], cov=[[2,1], [1,2]], size=200)\n",
    "B = np.random.multivariate_normal(mean=[-2, -2], cov=[[2,0], [0,2]], size=200)\n",
    "\n",
    "#get them into proper matrix form\n",
    "X = np.vstack([A, B])\n",
    "Y = np.hstack([np.zeros(len(A)), np.ones(len(B))])\n",
    "\n",
    "# train the linear regressor and save the coefficents\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, Y)\n",
    "b_1, b_2 = reg.coef_\n",
    "b_0 = reg.intercept_\n",
    "\n",
    "# solve the function y = b_0 + b_1*X_1 + b_2 * X_2 for X2\n",
    "x1s = np.linspace(-8, 8)\n",
    "x2s = (0.5 - b_0 - b_1*x1s)/b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_exercise_1(A, B, x1s, x2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We just *learned* the parameters for a statistical model based on labeled data.\n",
    "\n",
    "Can a linear classification improve the classification of the Titanic dataset case?\n",
    "\n",
    "We have to evaluate our 'learned' model independent test set\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[['Sex_Code', 'Pclass_Code', 'Fare', 'Age']]\n",
    "y = data['Survived_Code']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6)\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_prediction = reg.predict(X_test)\n",
    "y_prediction = np.where(y_prediction > 0.5, 1, 0)\n",
    "\n",
    "plots.plot_bars_and_confusion(truth=y_test, prediction=y_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The model does not seem to improve the classification to a large degree. \n",
    "\n",
    "\n",
    "We will talk more about properly validating models later. \n",
    "\n",
    "\n",
    "For now lets look at the data in three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "set_mpl()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(data.Fare, data.Age, data.Sex_Code, c=data.Survived_Code, cmap='winter')\n",
    "ax.set_xlabel('Fare / $')\n",
    "ax.set_ylabel('Age / Years')\n",
    "ax.set_zlabel('Sex')\n",
    "ax.set_zticks([0,1])\n",
    "ax.set_zticklabels(['Male', 'Female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_sns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## SVMs\n",
    "\n",
    "The basic assumption underlying the least squares approach is that the model is linear in the observed variables. \n",
    "This works for data which can be separated by a linear function (a hyperplane in the parameter space).\n",
    "\n",
    "But how doe we know that this method finds the 'best' hyperplane for separating the two classes?\n",
    "\n",
    "And what if the data cannot be seperated by plane?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# many possible lines to separate the data. Which one is 'better'?\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=150, centers=2,\n",
    "                  random_state=3, cluster_std=0.70)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='winter')\n",
    "\n",
    "xs = np.linspace(-6.5, 3, 2)\n",
    "plt.plot(xs, -2 * xs - 2, color='gray', linestyle='--')\n",
    "plt.plot(xs, -0.4 * xs + 2, color='gray', linestyle='--')\n",
    "plt.xlim([-6, 3])\n",
    "plt.ylim([-2, 6])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Support Vector Machine\n",
    "\n",
    "Again we minimze a loss function.\n",
    "\n",
    "$$\n",
    "L(\\beta) = C \\max(0, 1 - y_i \\beta^T x_i) + \\frac{\\lambda}{2}||{\\beta}||^2\n",
    "$$\n",
    "\n",
    "Support Vector Machines try to find the hyperplane which maximimizes the margin to the points in different classes in the parameter space.\n",
    "\n",
    "$C$ and $\\lambda$ are two parameters which can be chosen beforehand. \n",
    "\n",
    "<p style=\"color:gray\"> Note that, to fit the definition above, the label encoding has to be $y_i \\in {-1, 1}$</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=150, centers=2,\n",
    "                  random_state=3, cluster_std=0.70)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='winter')\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, y)\n",
    "\n",
    "plots.draw_svm_decission_function(clf, colors='black', label='SVM')\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "plots.draw_linear_regression_function(reg, label='Linear Regression', color='gray', alpha=0.5)\n",
    "\n",
    "plt.xlim([-6, 3])\n",
    "plt.ylim([-2, 6])\n",
    "plt.legend(loc='lower right', frameon=True, framealpha=0.95, facecolor='white')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So far the data has still been separable by a linear function. \n",
    "\n",
    "For many problems in real life however this isn't the case. \n",
    "\n",
    "Heres an example of (artificial) data which cannot be seperated by a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.10)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='winter')\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, y)\n",
    "plots.draw_svm_decission_function(clf, colors='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if we take that data and transform it into a new variable. \n",
    "\n",
    "Find a function $h$ to create a new variable $X_h = h(X_1, X_2, \\ldots)$.\n",
    "\n",
    "In the case above some radial symmetry seems be an underlying feature of the data. \n",
    "\n",
    "We can exploit that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "set_mpl()\n",
    "\n",
    "# add a dimension by applying a transformation on the two variables in the data. \n",
    "r = np.exp(-(X[:, 0] ** 2 + X[:, 1] ** 2))\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='winter')\n",
    "ax.view_init(elev=45, azim=45)\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_zlabel('r')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='winter')\n",
    "ax.view_init(elev=5, azim=70)\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_zlabel('r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "set_sns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.10)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='winter')\n",
    "\n",
    "clf = SVC(kernel='rbf') #use the radial basis function instead of the linear one.\n",
    "clf.fit(X, y)\n",
    "plots.draw_svm_decission_function(clf, colors='black', label='SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The same approach works for other linear methods as well. \n",
    "\n",
    "What makes SVM's so special?:\n",
    "\n",
    "+ SVM's have proven to perform very well for many use-cases.\n",
    "\n",
    "+ SVM's handle large number of dimensions relativly fast.\n",
    "\n",
    "+ The kernel functions basically come for free.\n",
    "\n",
    "+ Easily extendable to multi-class problems.\n",
    "\n",
    "\n",
    "Kernel functions are constrained to fulfill certain criteria. *(See Chapter 12.3.1 in the Hastie Book)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Excersise 2: SVM and Titanic (10  minutes)\n",
    "\n",
    "Use scikit-learn's SVC implementation on the Titanic dataset.\n",
    "\n",
    "1. Read the dataset using Pandas and convert non numeric types (string etc.. to numbers) and drop missing data.\n",
    "\n",
    "        data = pd.read_csv('../resources/titanic_train.csv', index_col='PassengerId').dropna(subset=['Age'])\n",
    "        ...\n",
    "            \n",
    "2. Create the $X$ matrix using 'Sex,Fare,Age,Class' and the output (label) vector $Y$\n",
    "        \n",
    "        X = data[[...]]\n",
    "        Y = data['Survived']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "        \n",
    "3. Use scikit-learn's SVC with the 'linear' and another kernel function\n",
    "\n",
    "        reg_linear = SVC(kernel='linear')\n",
    "        reg_linear.fit...\n",
    "        reg_linear.predict...\n",
    "        \n",
    "4. Plot the number of correctly and falsely predicted events for both predictors.\n",
    "\n",
    "        from sklearn.metric import confusion_matrix\n",
    "        m = confusion_matrix(y_true, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ml_helpers import plots\n",
    "\n",
    "data = read_titanic()\n",
    "\n",
    "X = data[['Sex_Code', 'Pclass_Code', 'Fare', 'Age']]\n",
    "y = data['Survived_Code']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "\n",
    "#Use linear kernel\n",
    "reg = SVC(kernel='linear')\n",
    "reg.fit(X_train, y_train)\n",
    "prediction_linear = reg.predict(X_test)\n",
    "\n",
    "#Use the rbf kernel\n",
    "reg_rbf = SVC(kernel='rbf')\n",
    "reg_rbf.fit(X_train, y_train)\n",
    "prediction_rbf = reg_rbf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2, 2, figsize=(10, 10))\n",
    "plots.plot_bars_and_confusion(truth=y_test, prediction=prediction_linear, axes=[ax1, ax2], vmin=0, vmax=182)\n",
    "plots.plot_bars_and_confusion(truth=y_test, prediction=prediction_rbf, axes=[ax3, ax4], vmin=0, vmax=182)\n",
    "ax1.set_title('Linear Kernel')\n",
    "ax3.set_title('Radial Kernel')\n",
    "ax1.set_xlim([0, 300])\n",
    "ax3.set_xlim([0, 300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Extending SVMs to more dimensions\n",
    "\n",
    "The Titanic dataset we looked at in the previous excercise had four observed variables or dimensions and 714 observations in total. \n",
    "\n",
    "        > X = data[['Sex_Code', 'Pclass', 'Fare', 'Age']]\n",
    "        > X.shape\n",
    "        (714, 4)\n",
    "\n",
    "Now we take a dataset that has 64 variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "X, y = datasets.load_digits( return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "#SVC with default settings.\n",
    "clf = SVC(kernel='poly')\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Now predict the value of the digit on the test sample\n",
    "y_prediction = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_prediction)\n",
    "\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='RdPu',\n",
    ")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__What mystery data did we just classifiy?__\n",
    "\n",
    "The SVM can easily classifiy a dataset of many observables and target classes.\n",
    "\n",
    "This data set had 64 observables and 10 different classes.\n",
    "\n",
    "Lets take the 64 numbers in the single observations and plot them into a $8\\times8$ grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm, metrics\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "plt.figure(figsize=(15, 1))\n",
    "plt.imshow([X[0]], aspect='auto', cmap='gray_r',)\n",
    "plt.grid('off')\n",
    "plt.yticks([])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "fig, axs = plt.subplots(1, 10, figsize=(10, 10))\n",
    "for i, x_i in enumerate(X[:10]):\n",
    "    ax = axs[i]\n",
    "    img = x_i.reshape(-1, 8)\n",
    "    ax.imshow(img, cmap='gray_r', interpolation='nearest')\n",
    "    ax.grid('off')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This classifier just performed __character recognition__ on raw image inputs without any feature engineering.\n",
    "\n",
    "Currently there is a lot of buzz (or even hype) about image recognition tasks and neural networks (Deep Learning etc.)\n",
    "Neural networks and SVMs are very similar in nature they just use slightly modified loss functions.\n",
    "\n",
    "See Stanfords computer science lecture CS231n for more information, especially the chapter on linear classification.\n",
    "\n",
    "[http://cs231n.github.io](http://cs231n.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local Optimization and Decission Trees\n",
    "\n",
    "So far we looked at loss functions which optimized some global optimization criterion.\n",
    "\n",
    "In cases of non-linearity some a priori knowledge is necessary to transform the data to make it seperable by a hyperplane. (or you can use *Deep Learning*)\n",
    "\n",
    "\n",
    "Idea:\n",
    "* Split the parameter space into many subspaces where observations of the same class live.\n",
    "\n",
    "Problem:\n",
    "* Finding the *best* set of subspaces in the parameter space is an NP-complete problem (Its hard to solve. Really hard.)\n",
    "\n",
    "One can however try approximate the solution using binary recursive splits in the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.10)\n",
    "clf = DecisionTreeClassifier(max_depth=2, criterion='entropy')\n",
    "clf.fit(X, y)\n",
    "\n",
    "d = tree.export_graphviz(\n",
    "                clf, \n",
    "                out_file=None,   \n",
    "                filled=True,  \n",
    ") \n",
    "graphviz.Source(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='winter')\n",
    "plots.draw_decission_boundaries(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline for a tree building algorithm.\n",
    "\n",
    "        def build_tree(space)\n",
    "            if stopping_criterion_fulfilled():\n",
    "                return {}\n",
    "             \n",
    "            variable, split_point = find_best_split(space)\n",
    "                        \n",
    "            left, right = split_space(space, variable, split_point)\n",
    "\n",
    "            left_tree = build_tree(left)\n",
    "            right_tree = build_tree(right)\n",
    "               \n",
    "            return {'node' : (variable, split_point), 'left': left_tree, 'right': right_tree}\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification the best split in a node $m$ of the tree is found by minimizing an impurity measure $Q_m$.\n",
    "\n",
    "Popular ones include Information Gain, Cross-Entropy or the Gini index. \n",
    "\n",
    "They all work by looking at one variable at a time and then iterating over all the possible splits to find the minimal $Q_m$\n",
    "\n",
    "Implementations across languages/libraries are similar but differ in their choice of $Q_m$ and handling of continous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Excersise 3: Decission Trees and Titanic (15  minutes)\n",
    "\n",
    "Use scikit-learn to find the best possible decission tree for the Titanic dataset.\n",
    "\n",
    "1. Read the dataset using Pandas.\n",
    "2. Split into training and test data.\n",
    "3. Create and instance scikit-learn's DecissionTreeClassifier.\n",
    "4. Train one decission tree for all combinations of criterion = ['entropy', 'gini'] and max_depth=[1, .., 20]\n",
    "5. Plot the accuracy of each predictor into a heatmap-like figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1235)\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "data = read_titanic()\n",
    "\n",
    "X = data[['Sex_Code', 'Pclass_Code', 'Fare', 'Age']]\n",
    "y = data['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "df =  pd.DataFrame()\n",
    "ps = ParameterGrid({'max_depth' : range(1, 20), 'criterion' : ['entropy', 'gini']})\n",
    "for d in ps:\n",
    "    clf = DecisionTreeClassifier(max_depth=d['max_depth'], criterion=d['criterion'])\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    df = df.append({'max_depth': d['max_depth'], 'criterion': d['criterion'], 'accuracy': acc}, ignore_index=True)\n",
    "\n",
    "df = df.pivot('max_depth', 'criterion', 'accuracy')\n",
    "sns.heatmap(df, cmap='YlOrRd', annot=True, fmt='.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=3, criterion='entropy')\n",
    "clf.fit(X_train, y_train)\n",
    "d = tree.export_graphviz(\n",
    "                clf, \n",
    "                out_file=None,   \n",
    "                filled=True,\n",
    "                class_names=['Survived', 'Deceased'],\n",
    "                feature_names=['Sex', 'BoardClass', 'Fare', 'Age'],\n",
    "                precision=2,\n",
    ") \n",
    "graphviz.Source(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classifier Validation \n",
    "\n",
    "### k - Nearest Neighbour Methods\n",
    "\n",
    "The k-NN classifier is  a good example of a model that can be easily overfitted. \n",
    "\n",
    "Lets assume that the decission function is constant over some local region in the parameter space:\n",
    "\n",
    "$$\n",
    "\\hat{f}(x_0) = \\hat{y} = \\frac{1}{k} \\sum_{x_i \\in N_k(x_o)} y_i\n",
    "$$\n",
    "\n",
    "where $x_i \\in N_k(x)$ describes the $k$ points in the training data $\\mathbf{X}$ that are in the *neighbourhood* of $x_0$.\n",
    "\n",
    "To put it in words. We assume $x$ will have the same $y$ as other points nearby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_moons\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=25)\n",
    "knn.fit(X, y)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='winter')\n",
    "plots.draw_decission_boundaries(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works pretty well in this artifical, low-noise, example. \n",
    "\n",
    "Classification on noisy data will not work as perfectly.\n",
    "\n",
    "In the real world, all data has some form of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.2)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=40)\n",
    "knn.fit(X, y)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='winter')\n",
    "plots.draw_decission_boundaries(knn, ax=ax1)\n",
    "ax1.set_title('Accuracy for k=40 : {}'.format(accuracy_score(y, knn.predict(X))))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)\n",
    "\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='winter')\n",
    "plots.draw_decission_boundaries(knn, ax=ax2)\n",
    "ax2.set_title('Accuracy for k=1 : {}'.format(accuracy_score(y, knn.predict(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting (Bias-Variance Tradeoff)\n",
    "\n",
    "Assume the target $y$ is generated by some function $f(x)$ with added gaussian noise $\\epsilon$\n",
    "\n",
    "$$\n",
    "y = f(x) + \\epsilon, \\qquad \\epsilon \\propto \\mathcal{N}(\\mu=0, \\sigma)\n",
    "$$\n",
    "\n",
    "The mean squared error (mse) of the predictor function $\\hat{f}(x) = \\hat{y}$ is\n",
    "\n",
    "$$\n",
    "mse(y, \\hat{f}) = (y - \\hat{f}(x))^2 .\n",
    "$$\n",
    "\n",
    "Calculate the expectation value $mse$ \n",
    "\n",
    "$$\n",
    "E[mse(y, \\hat{f}) ] = E[(y - \\hat{f}(x))^2]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some mathematical definitions up front.\n",
    "\n",
    "__Variance__ of a random variable $X$ \n",
    "  \n",
    "  $$\n",
    "  Var(X) = E[(X - E[X])^2] = E[X^2] - E[X]^2 \\iff     E[X^2] = Var[X] +  E[X]^2 \n",
    "  $$\n",
    "  \n",
    "__Bias__ of an estimator $\\hat{f}$\n",
    "\n",
    "  $$\n",
    "  Bias(\\hat{f}) = E[\\hat{f} - f] = E[\\hat{f}] - E[f] = E[\\hat{f}] - f\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $f$ is a fixed function\n",
    "\n",
    "$$\n",
    "E[f] = f\n",
    "$$\n",
    "\n",
    "Using these definition on $y$ gives\n",
    "\n",
    "\\begin{align}\n",
    "Var[y] = Var[ f(x) + \\epsilon]  & = Var[f(x)] + Var[\\epsilon] \\\\\n",
    "                                & = Var[f(x)] + \\sigma^2 \\\\\n",
    "                                & = E[f(x)^2] - E[f(x)]^2 + \\sigma^2 \\\\\n",
    "                                & = f(x)^2 - f(x)^2 + \\sigma^2 \\\\\n",
    "                                & = \\sigma^2 \n",
    "\\end{align}\n",
    "\n",
    "Finally calculating $E[mse(y, \\hat{f})]$ yields\n",
    "\n",
    "\\begin{align}\n",
    " E[(y - \\hat{f}(x))^2]  & = E[y^2 + \\hat{f}^2 - 2y\\hat{f}] \\\\\n",
    "                        & \\ldots \\\\\n",
    "                        & = \\sigma^2 + Var[\\hat{f}] + Bias[\\hat{f}]^2.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the k-NN classifier there exists an analytical expression for $E[mse(y, \\hat{f})]$\n",
    "Using \n",
    "$$\n",
    "\\hat{f}(x) = \\hat{y} = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i\n",
    "$$\n",
    "\n",
    "and again assuming the data is subject to normally distributed noise $\\epsilon$\n",
    "\n",
    "$$\n",
    "y_i = f(x_i) + \\epsilon_i\n",
    "$$\n",
    "\n",
    "calculate \n",
    "$$\n",
    " E[(y - \\hat{f}(x))^2] = \\sigma^2 + Var[\\hat{f}] + Bias[\\hat{f}]^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with $Var(\\hat{f})$\n",
    "\n",
    "\\begin{align}\n",
    "Var(\\hat{f(x_0)}) &= Var \\left( \\frac{1}{k} \\sum_{y_i \\in N_k(x_0)} y_i \\right)  \\\\\n",
    "&= \\frac{1}{k^2}  \\sum_{y_i \\in N_k(x_0)} Var \\left( f(x_i) + \\epsilon_i \\right)  \\\\\n",
    "&= \\frac{1}{k^2}  \\sum_{y_i \\in N_k(x_0)} Var \\left( f(x_i) \\right) + Var \\left( \\epsilon_i \\right)  \\\\\n",
    "&= \\frac{1}{k^2}  \\sum_{y_i \\in N_k(x_0)} Var \\left( \\epsilon_i \\right)  \\\\\n",
    "&= \\frac{1}{k^2} k \\sigma^2  \\\\\n",
    "&= \\frac{\\sigma^2}{k}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the bias term\n",
    "\n",
    "\\begin{align}\n",
    "    Bias^2(\\hat{f}) & = \\left( E[\\hat{f} - y] \\right)^2 \\\\\n",
    "                    & = \\left( E[\\frac{1}{k} \\sum_{y_i \\in N_k(x_0)} y_i - y] \\right)^2 \\\\\n",
    "                    & = \\left( E[\\frac{1}{k} \\sum_{y_i \\in N_k(x_0)} f(x_i) + \\epsilon_i - ( f(x_0) + e_0 )] \\right)^2 \\\\\n",
    "                    & = \\left( \\frac{1}{k} \\sum_{y_i \\in N_k(x_0)} f(x_i) - f(x_0) \\right)^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it together gives \n",
    "\n",
    "$$\n",
    " E_{knn}[(y - \\hat{f}(x))^2] = \\sigma^2 + Var[\\hat{f}] + Bias[\\hat{f}]^2 = \\sigma^2 + \\frac{\\sigma^2}{k} + \\left( \\frac{1}{k} \\sum_{y_i \\in N_k(x_0)} f(x_i) - f(x_0) \\right)^2\n",
    "$$\n",
    "\n",
    "The expectation of mean squared error depends on the choice of $k$ and the noise of the data $\\sigma$. \n",
    "\n",
    "When $k$ increases the dependency on the data noise decreases. \n",
    "\n",
    "The bias increases when choosing larger values for $k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This so called Bias-Variance dillemma is a universal problem in supervised machine learning. \n",
    "\n",
    "There are two error sources:\n",
    "\n",
    "* High bias might cause *smoothing* out relations between data and target. \n",
    "* High variance can make the learned paramters prone to noise in the training data.  \n",
    "\n",
    "If the parameters are tuned to the noise in the training data, the model will not generalize to new data. \n",
    "\n",
    "This problem is called __overfitting__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.7)\n",
    "X_test, y_test = make_moons(n_samples=50, noise=0.7)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='winter')\n",
    "plots.draw_decission_boundaries(knn, ax=ax1)\n",
    "ax1.set_title('Accuracy on Training Data for k=40 : {}'.format(accuracy_score(y, knn.predict(X))))\n",
    "\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=30, cmap='winter')\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=y, s=15, alpha=0.1,  cmap='winter')\n",
    "plots.draw_decission_boundaries(knn, ax=ax2)\n",
    "ax2.set_title('Accuracy on Test Sample for k=1 : {}'.format(accuracy_score(y_test, knn.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(1234)\n",
    "X, y = make_moons(n_samples=400, noise=0.2)\n",
    "X_test, y_test = make_moons(n_samples=400, noise=0.2)\n",
    "\n",
    "e_train = []\n",
    "e_test = []\n",
    "for k in range(1, 200):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X, y)\n",
    "    e_train.append(mean_squared_error(y, knn.predict(X)))\n",
    "    e_test.append(mean_squared_error(y_test, knn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 200), e_train,'.', color='#FF6B6B', ms=10, label='Training Sample')\n",
    "plt.plot(range(1, 200), e_test, '.' ,color='#FFAE6B', ms=10, label='Test Sample', )\n",
    "plt.xlim(200, 0)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Prediction Error')\n",
    "\n",
    "plt.text(150, 0.026, 'Increasing Model Complexity')\n",
    "plt.arrow(150, 0.018, -50, 0, width = 0.0005, head_width=0.003, head_length=3, fc='k', ec='k')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Validation on independent test sets\n",
    "\n",
    "Validating the fitted models is essential for avoiding overfitting.\n",
    "\n",
    "The predictions error has to be assesed on an independent test dataset. \n",
    "\n",
    "Models might still be susceptible to noise in the training data however.\n",
    "\n",
    "#### Cross Validation\n",
    "\n",
    "A $k$-fold cross validation automatically splits the training data into $k$ subsets.\n",
    "\n",
    "The model is then trained on $k-1$ subsets and evaluated on the remaining set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "    <style>\n",
    "    .training_set { fill: #FF6B6B;}\n",
    "    .test_set { fill: #FFAE6B; }\n",
    "    </style>\n",
    "<h4> 5-Fold Cross Validation </h4>\n",
    "<p> First Iteration: </p>\n",
    "<p> </p>\n",
    "<svg width=\"800\" height=\"140\">\n",
    "<g transform=\"scale(0.9)\">\n",
    "  <rect x=\"0\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"160\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"320\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"480\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"640\", y=\"0\", width=\"150\" height=\"80\" class=\"test_set\" />\n",
    "    \n",
    "  <rect x=\"0\", y=\"90\", width=\"630\" height=\"3\" class=\"training_set\" />\n",
    "  <rect x=\"640\", y=\"90\", width=\"150\" height=\"3\" class=\"test_set\" />\n",
    "    \n",
    "  <text x=\"0\" y=\"115\" class=\"training_set\">\n",
    "    Training Data\n",
    "  </text>\n",
    "  <text x=\"640\" y=\"115\" class=\"test_set\">\n",
    "    Test Data\n",
    "  </text>\n",
    "</g>\n",
    "</svg>\n",
    "\n",
    "<p> Second Iteration: </p>\n",
    "<p> </p>\n",
    "<svg width=\"800\" height=\"140\">\n",
    "<g transform=\"scale(0.9)\">\n",
    "  <rect x=\"0\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"160\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"320\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"480\", y=\"0\", width=\"150\" height=\"80\" class=\"test_set\" />\n",
    "  <rect x=\"640\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "    \n",
    "  <rect x=\"0\", y=\"90\", width=\"470\" height=\"3\" class=\"training_set\" />\n",
    "  <rect x=\"480\", y=\"90\", width=\"150\" height=\"3\" class=\"test_set\" />\n",
    "  <rect x=\"640\", y=\"90\", width=\"150\" height=\"3\" class=\"training_set\" />\n",
    "    \n",
    "  <text x=\"0\" y=\"115\" class=\"training_set\">\n",
    "    Training Data\n",
    "  </text>\n",
    "  <text x=\"480\" y=\"115\" class=\"test_set\">\n",
    "    Test Data\n",
    "  </text>\n",
    "</g>\n",
    "</svg>\n",
    "\n",
    "<p> Third Iteration: </p>\n",
    "<p> </p>\n",
    "<svg width=\"800\" height=\"140\">\n",
    "<g transform=\"scale(0.9)\">\n",
    "  <rect x=\"0\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"160\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"320\", y=\"0\", width=\"150\" height=\"80\" class=\"test_set\" />\n",
    "  <rect x=\"480\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"640\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "    \n",
    "  <rect x=\"0\", y=\"90\", width=\"310\" height=\"3\" class=\"training_set\" />\n",
    "  <rect x=\"320\", y=\"90\", width=\"150\" height=\"3\" class=\"test_set\" />\n",
    "  <rect x=\"480\", y=\"90\", width=\"310\" height=\"3\" class=\"training_set\" />\n",
    "    \n",
    "  <text x=\"0\" y=\"115\" class=\"training_set\">\n",
    "    Training Data\n",
    "  </text>\n",
    "  <text x=\"320\" y=\"115\" class=\"test_set\">\n",
    "    Test Data\n",
    "  </text>\n",
    "</g>\n",
    "</svg>\n",
    "<p>...</p>\n",
    "<p>...</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Quality Metrics\n",
    "\n",
    "Accuracy is not always a good measure of model quality.\n",
    "\n",
    "Imagine a classifier function which simply predicts a fixed outcome.\n",
    "\n",
    "        def f_fixed(x):\n",
    "            return 0\n",
    "            \n",
    "On an imbalanced dataset this classifier will have an accuracy equal to the ratio of positive examples to the total number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_fixed(x):\n",
    "    return np.zeros(shape=len(x))\n",
    "\n",
    "data = read_titanic()\n",
    "\n",
    "X = data\n",
    "y = data['Survived_Code']\n",
    "\n",
    "print('Accuracy of fixed classifier {:1.6f} \\n'.format(accuracy_score(y, f_fixed(X))))\n",
    "\n",
    "print('Ratio of survived to total passengers: ')\n",
    "print(data.Survived.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another view at the confusion matrix. It generalizes to more than two classes as seen in the picture below.\n",
    "\n",
    "![A confusion matrix](./ml/images/confusion_matrix.png)\n",
    "\n",
    "The numbers in the confusion matrix can be used to calculate a whole range of qulity criteria.\n",
    "\n",
    "Lets build a classifier which randomly chooses an outcome and look at the different criteria.\n",
    "\n",
    "```\n",
    "def f_random(passenger):\n",
    "    return np.random.choice(['yes', 'no')\n",
    "```\n",
    "\n",
    "\n",
    "<a href=\"https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\" style=\"color:#BBBBBB;\">The nice Stack Overflow post where I stole the picture from.</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_random(x):\n",
    "    return np.random.randint(2, size=len(x))\n",
    "\n",
    "prediction = f_random(X)\n",
    "cm = confusion_matrix(y, prediction)\n",
    "\n",
    "FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print(TPR, TNR, PPV, NPV, FPR, FNR, FDR, ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score\n",
    "print(recall_score(y, prediction))\n",
    "print(precision_score(y, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precission and recall can be understood in an intuitive way\n",
    "\n",
    "* __Recall__ How many of the wanted examples are found.\n",
    "* __Precission__ The percentage of the found examples that are actually relevant.\n",
    "\n",
    "So what should you optimze for? Maximumg accuracy or precission or recall?\n",
    "\n",
    "There is no clear answer. It depends on your use-case. Can you tolerate false positives? \n",
    "Can you tolerate losing some true positives?\n",
    "\n",
    "#### Excersise 4 (Discussion)\n",
    "\n",
    "Imagine you devise a new cheap and easy cancer test. \n",
    "What should optimize your decission threshold for?\n",
    "Precission or recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will always have to make the trade-off between recall and precission. \n",
    "There are several metrics which try to combine both into one.\n",
    "\n",
    "The $f_{\\beta}$ score is one example.\n",
    "\n",
    "$$\n",
    "f_{\\beta } = (1+\\beta ^{2})\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{(\\beta ^{2}\\cdot \\mathrm {precision} )+\\mathrm {recall} }} =\n",
    "\\frac {(1 + \\beta^2) \\cdot TP }{(1 + \\beta^2) \\cdot TP + \\beta^2 \\cdot FN + FP}.\n",
    "$$\n",
    "\n",
    "But in the end there is no absolute truth to whats best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excersise 5 (10 Minutes)\n",
    "\n",
    "scikit-learn comes with several functions to make working with cross validations easy.\n",
    "\n",
    "1. Read the Titanic dataset into a dataframe. \n",
    "2. Calculate cross validated accuracy, recall, precision and f1 score using `sklearn.model_selection.cross_validate` with a 5-fold CV for a k-NN, SVM and DecissionTree classifier.\n",
    "3. Print or plot the mean and standard deviation of the accuracies for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "from sklearn.model_selection import cross_validate\n",
    "data = read_titanic()\n",
    "\n",
    "X = data[['Sex_Code', 'Pclass_Code', 'Fare', 'Age']]\n",
    "y = data['Survived_Code']\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "results = []\n",
    "for clf, name in zip([svc, knn, tree], ['SVM', 'kNN', 'tree']):\n",
    "    r = cross_validate(clf, X=X, y=y, cv=5, scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
    "    df = pd.DataFrame().from_dict(r)\n",
    "    df['classifier'] = name\n",
    "    results.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(results).drop(['fit_time', 'score_time'], axis='columns')\n",
    "\n",
    "means = df.groupby('classifier').mean()\n",
    "deviations = df.groupby('classifier').std()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "sns.heatmap(means, cmap='viridis', annot=True, ax=ax1, vmin=0, vmax=1)\n",
    "sns.heatmap(deviations, cmap='viridis', annot=True, ax=ax2, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decission Thresholds and Classifier Validation\n",
    "\n",
    "Classifier objects usually provide all of the following methods\n",
    "\n",
    "* `classifier.fit(X)` takes training data and finds some parameters based on that data.\n",
    "* `classifier.predict(X_new)` takes new data (one row or many) and predicts the target label for each row.\n",
    "* `classifier.predict_proba(X_new)` takes new data (one row or many) and predicts 'some notion of confidence'.\n",
    "\n",
    "In the case of binary classification (i.e. two classes) the `classifier.predict_proba` usually returns a number where higher numbers indicate some measure of 'confidence'.\n",
    "\n",
    "The `classifier.predict(X_new)` is basically a a wrapper around the `predict_proba` function which simply applies a decission threshold at some value (usually 0.5).\n",
    "This is exactly what we did in the case of linear least squares regression.\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\begin{cases}\n",
    "\\text{Yes}, & \\text{if $ f(X) \\gt 0.5$} \\\\\n",
    "\\text{No}, & \\text{if $ f(X) \\le 0.5$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In this case the decission threshold corresponds to the distance of a point to the seperating hyperplane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_exercise_1(A, B, x1s, x2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what does this say about the actual probability of a new data point being of either class?\n",
    "\n",
    "In the case above, data created by two gaussian distributions, the distance certainly maps to the underlying probability density in *some* way. But it is in no way an actual *significance* or *confidence*\n",
    "\n",
    "The function `predict_proba` is a slight misnomer. While some classifiers return numbers between 0 and 1, by no means do all classifier return the desired probability estimate.\n",
    "\n",
    "Still the number can be interpreted as some level of 'certainty' in many cases.\n",
    "\n",
    "Varying the decission threshold is extremely usefull for modifying your classifier output to create more/less 'conservative' predictions.\n",
    "\n",
    "\n",
    "In essence this is a new classifier/predictor with a free parameter. The old $\\hat{y} = \\hat{f}(x)$ now becomes\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{f}(x, \\alpha)\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a parameter which can be chosen freely (or optimized according to some criterion which has nothing to do with the underlying loss function of the predictor.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1500, noise=0.6)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=2, cmap='winter_r')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "plots.draw_decission_surface(clf, predictions, label=r'$ \\alpha $')\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Calibration\n",
    "\n",
    "There are ways to transform the output of a classifiers into more reasonable probability estimates. \n",
    "\n",
    "This process is often called classifier calibration. there is a detailed guid in sklearn's documentation\n",
    "\n",
    "http://scikit-learn.org/stable/modules/calibration.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receiver Operating Characteristic \n",
    "\n",
    "The decission threshold is incredibly helpful in validating classifier performance.\n",
    "\n",
    "The plot of false positive rate vs. true positive rate while varying the decission threshold is called the Receiver Operating Characteristic curve (ROC curve).\n",
    "\n",
    "It is a very popular tool when evaluating classifier performance.\n",
    "\n",
    "Wikipedia Quote:\n",
    "\n",
    "> Following the attack on Pearl Harbor in 1941, the United States army began new research to increase the prediction of correctly detected Japanese aircraft from their radar signals. For this purposes they measured the ability of radar receiver operators to make these important distinctions, which was called the Receiver Operating Characteristics.\n",
    "\n",
    "\n",
    "A classifier which assigns random labels to the data will have a ROC curve which lies on the diagonal. With an area under curve (AUC) of 0.5.\n",
    "\n",
    "\n",
    "__Problems__\n",
    "\n",
    "There are circumstance in which the ROC is not a good measure of quality. \n",
    "\n",
    "See https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve for a discussion.\n",
    "\n",
    "Also in the case of imbalanced classes (eg. imagine you had 10000 surviors but 10 deceased passengers) the ROC curve itself won't change. The intepretation of the ROC curve however changes drastically.\n",
    "\n",
    "In that case its better to plot the precission vs recall curve and the corresponding area under curve.\n",
    "\n",
    "See https://classeval.wordpress.com for some very good discussions on classifier evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "X, y = make_classification(n_samples=1000)\n",
    "\n",
    "prediction = np.random.uniform(size=len(y))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y, prediction)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(fpr, tpr, '.')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Excersise 6. Validation Curves (15 minutes)\n",
    "\n",
    "1. Create some artificial noisy data.\n",
    "\n",
    "        X, y = make_moons(n_samples=5000, noise=0.8)\n",
    "\n",
    "2. Create a classifier\n",
    "        \n",
    "        # either\n",
    "        clf = KNeighborsClassifier(n_neighbors=50)\n",
    "        # or\n",
    "        clf = DecisionTreeClassifier(min_samples_leaf=50))\n",
    "        \n",
    "3. Create an cross validation iterator\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=5)\n",
    "        \n",
    "4. Iterate over the splits in the cross validation\n",
    "\n",
    "        for train, test in cv.split(X, y):\n",
    "            X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "\n",
    "5. Plot recall vs. precission and fpr vs tpr for each split in the cross validation\n",
    "\n",
    "        # use these two functions\n",
    "        from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "        \n",
    "6. Calculate the mean and standard deviation of the area under both curves.\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.datasets import make_moons\n",
    "from matplotlib import patches\n",
    "\n",
    "X, y = make_moons(n_samples=5000, noise=0.9)\n",
    "\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=50)\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "roc_auc = []\n",
    "pr_auc = []\n",
    "\n",
    "for train, test in cv.split(X, y):\n",
    "    X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    prediction = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    p, r, thresholds_pr = precision_recall_curve(y_test, prediction)\n",
    "    fpr, tpr, thresholds_roc = roc_curve(y_test, prediction)\n",
    "    \n",
    "    roc_auc.append(roc_auc_score(y_test, prediction))\n",
    "    pr_auc.append(average_precision_score(y_test, prediction))\n",
    "\n",
    "    ax1.step(thresholds_pr, r[: -1], color='gray', where='post')\n",
    "    ax1.step(thresholds_pr, p[: -1], color='darkgray', where='post')\n",
    "      \n",
    "    ax2.step(r, p, color='darkmagenta', where='post')\n",
    "\n",
    "    ax3.step(thresholds_roc, tpr, color='gray', where='post')\n",
    "    ax3.step(thresholds_roc, fpr, color='darkgray', where='post')\n",
    "    \n",
    "    ax4.step(fpr, tpr, color='mediumvioletred', where='post')\n",
    "\n",
    "\n",
    "\n",
    "p1 = patches.Patch(color='gray', label='Recall')\n",
    "p2 = patches.Patch(color='darkgray', label='Precission')\n",
    "ax1.legend(handles=[p1, p2])\n",
    "ax1.set_xlabel('Decission Threshold')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_ylabel('Precission')\n",
    "ax2.set_xlabel('Recall')\n",
    "s = 'AUC {:0.3f} +/- {:0.3f}'.format(np.array(pr_auc).mean(), np.array(pr_auc).std())\n",
    "ax2.text(0.2, 0.2, s)\n",
    "\n",
    "p1 = patches.Patch(color='gray', label='True Positive Rate')\n",
    "p2 = patches.Patch(color='darkgray', label='False Positive Rate')\n",
    "ax3.legend(handles=[p1, p2])\n",
    "ax3.set_xlabel('Decission Threshold')\n",
    "ax3.set_xlim([0, 1])\n",
    "ax3.set_ylim([0, 1])\n",
    "\n",
    "ax4.set_xlim([0, 1])\n",
    "ax4.set_ylim([0, 1])\n",
    "ax4.set_ylabel('True Positive Rate')\n",
    "ax4.set_xlabel('False Positive Rate')\n",
    "s = 'AUC {:0.3f} +/- {:0.3f}'.format(np.array(roc_auc).mean(), np.array(roc_auc).std())\n",
    "ax4.text(0.2, 0.2, s)\n",
    "\n",
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "We have used a decission tree to classifiy artificial data as well as the Titanic data. \n",
    "\n",
    "Theoretically a decission tree is not limited in its depth. \n",
    "\n",
    "This quickly leads to overfitted tree models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=5000, noise=0.30)\n",
    "clf = DecisionTreeClassifier(max_depth=300, criterion='entropy')\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=3, cmap='winter')\n",
    "plots.draw_decission_boundaries(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the discussion about decission trees earlier, the tree building algorithms try to find the optimal split criterion in some local region of the parameter space.\n",
    "\n",
    "Finding the best overal split in parameter space is computationaly infeasible.\n",
    "\n",
    "This means the decission tree algorithm can run into a local optimum. \n",
    "\n",
    "The idea of _ensemble learning_ is to train several weak (high bias, low variance) base classifiers on different subsets of the data and then combine them into one big classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging\n",
    "\n",
    "A popular way to build ensembles is called *bagging*.\n",
    "\n",
    "Split the training data into $B$ subsets using sampling with replacement (Bootstrapping). For each subset $b$ we train a classifier $\\hat{f}_b$. Bagging then combines the overall prediction by taking the average.  \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}_b (x) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "\n",
    "Random Forests are a modification to bagging in which a number of *randomized decission trees* are trained. These randomized decission trees use a random subset of variables to find the best split in each node.\n",
    "\n",
    "        def build_random_tree(space)\n",
    "            if stopping_criterion_fulfilled():\n",
    "                return {}\n",
    "            \n",
    "            random_variable_choice = choose_random_selection_of_variables()\n",
    "            variable, split_point = find_best_split(space, random_variable_choice)\n",
    "                        \n",
    "            left, right = split_space(space, variable, split_point)\n",
    "\n",
    "            left_tree = build_tree(left)\n",
    "            right_tree = build_tree(right)\n",
    "               \n",
    "            return {'node' : (variable, split_point), 'left': left_tree, 'right': right_tree}\n",
    "\n",
    "Random Forests are a very popular choice for classification tasks since their parameters can be easily tuned and they often outperform other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = read_titanic()\n",
    "\n",
    "X = data[['Sex_Code', 'Pclass_Code', 'Fare', 'Age', 'SibSp']]\n",
    "y = data['Survived_Code']\n",
    "\n",
    "tree = DecisionTreeClassifier(min_samples_leaf=5)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=20, min_samples_leaf=5)\n",
    "\n",
    "score = cross_validate(tree, X, y, scoring=make_scorer(roc_auc_score), cv=5)\n",
    "print('ROC AUC Decission Tree {:0.3f} +/- {:0.3f}'.format(score['test_score'].mean(), score['test_score'].std()))\n",
    "\n",
    "score = cross_validate(rf, X, y, scoring=make_scorer(roc_auc_score), cv=5)\n",
    "print('ROC AUC Random Forest {:0.3f} +/- {:0.3f}'.format(score['test_score'].mean(), score['test_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Regression and classification are very similar in nature. The biggest difference being that the target variable $y$ is continous and has a natural ordering associated with it. \n",
    "\n",
    "The same basic rules for classification apply for regression as well. \n",
    "\n",
    "* Models need to be verified on independent test data\n",
    "* There is a tradeoff between bias and variance. Overfitting can occur.\n",
    "* There are many quality measures to pick from. \n",
    "\n",
    "\n",
    "Lets try and use regression to predict housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from IPython.display import Markdown, display\n",
    "houses = load_boston()\n",
    "\n",
    "display(Markdown(houses.DESCR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(houses['feature_names']) +  ['price']\n",
    "data = pd.DataFrame(data=np.c_[houses['data'], houses['target']], columns=names)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data[['INDUS', 'RM', 'NOX', 'AGE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "reg = LinearRegression()\n",
    "score = cross_validate(reg, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "print('Score linear regression {:0.3f} +/- {:0.3f}'.format(score['test_score'].mean(), score['test_score'].std()))\n",
    "\n",
    "reg = SVR(kernel='rbf')\n",
    "score = cross_validate(reg, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "print('Score SVR {:0.3f} +/- {:0.3f}'.format(score['test_score'].mean(), score['test_score'].std()))\n",
    "\n",
    "reg = DecisionTreeRegressor()\n",
    "score = cross_validate(reg, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "print('Score Tree {:0.3f} +/- {:0.3f}'.format(score['test_score'].mean(), score['test_score'].std()))\n",
    "\n",
    "reg = RandomForestRegressor()\n",
    "score = cross_validate(reg, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "print('Score RandomForestRegressor {:0.3f} +/- {:0.3f}'.format(score['test_score'].mean(), score['test_score'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "reg.fit(X_train, y_train)\n",
    "prediction = reg.predict(X_test)\n",
    "\n",
    "bin_edges = np.linspace(0, 60, 30)\n",
    "plt.hist2d(prediction, y_test, bins=bin_edges, cmap='viridis',)\n",
    "plt.colorbar()\n",
    "plt.grid()\n",
    "plt.plot([0, 60], [0, 60], color='gray')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a fundemental difference between *curve fitting* and regression.\n",
    "\n",
    "* __Curve Fitting__ All data is available. Some known (or presumed) analytical function is fit to the data to estimate free parameters of that function.\n",
    "\n",
    "\n",
    "* __Regression__ Training data is available. A model is fitted on training data to predict the dependent variable on some new, unknown, data.\n",
    "\n",
    "There is a lot more to learn about linar models and regression. Check sklearn's user guide on linear models for more information \n",
    "http://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n",
    "Another very popular regression metric is the $R^2$ score. Read about it here\n",
    "\n",
    "https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "\n",
    "and here\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (Unsupervised)\n",
    "\n",
    "## Mathematical Notation and Problem Description\n",
    "\n",
    "So far we have been occupied with \n",
    "predicting the values of one or more outputs or response variables $Y = (Y_1, \\ldots, Y_m)$ for a given set of input or predictor variables $X = (X_1, \\ldots , X_p)$. \n",
    "\n",
    "We have defined loss functions $L(y, \\hat y)$ to characterise how well we 'learned' some model $\\hat f(x)$.\n",
    "\n",
    "In unsupervised learning we have no given $Y$.\n",
    "These methods try to find the underlying (joint) probability density $Pr(X)$ so that we might learn some properties about it.\n",
    "\n",
    "One common question is whether $X$ is created by a mixture of two or more underlying random variables.\n",
    "\n",
    "One can visualize this problem by looking at the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# random number between 1 and 3\n",
    "k = np.random.randint(1, 4)\n",
    "X, y = make_blobs(n_samples=300, centers=k, center_box=(-2, 2), cluster_std=0.5)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can one infer $Pr(X)$ by looking at $X$, i.e. the blue dots?\n",
    "\n",
    "In this case we know that this distribution of blue dots, $X$, was created by joining $k$ two dimensional gaussians with known standard deviation.\n",
    "This is just what \n",
    "\n",
    "    X, y = make_blobs(n_samples=300, centers=k, center_box=(-2, 2), cluster_std=0.5)\n",
    " \n",
    "does.\n",
    "We even know in what region of space we have to look for the centroids of these blobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "cmap = matplotlib.cm.get_cmap('Set1')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=cmap(y))\n",
    "plt.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clustering Alogrithms\n",
    "\n",
    "Clustering alorithms try to find modes of $Pr(X)$ based on densities, neighbourhood relations or any other measure of 'similarity'  between points.\n",
    "\n",
    "More generally speaking to quote wikipedia again:\n",
    "\n",
    ">Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).\n",
    "\n",
    "\n",
    "### K-Means Algorithm\n",
    "\n",
    "The k-Means algorithm (or Loyds algorithm) tries to find a partition of the data into $k$ clusters $S = \\{S_1, \\ldots, S_k\\}$  which minimize the variance within those clusters. The number of clusters $k$ has to specified by the user.\n",
    "\n",
    "Formally speaking the algorithm solves\n",
    "$$\n",
    "{\\underset {S}{\\operatorname {arg\\,min} }}\\sum _{S_i \\in S}\\sum _{x \\in S_{i}}\\left\\|x -{\\overline{x}}_{S_i}\\right\\|^{2}.\n",
    "$$\n",
    "\n",
    "It does so iterativly according to the following steps \n",
    "\n",
    "1. Pick some initial cluster means (or centroids) $\\{m_1, \\ldots, m_k \\}$ either randomly or according to some heuristic.\n",
    "\n",
    "2. Create a partition $S$  by assigning each point $x \\in X$ to the cluster $S_i$ where the distance to $m_i$ is the smallest.\n",
    "\n",
    "3. Update the cluster means by calculating the means within the assigned clusters. \n",
    "\n",
    "4. Repeat steps 2 and 3 until convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=k, center_box=(-2, 2), cluster_std=0.5)\n",
    "\n",
    "prediction = KMeans(n_clusters=3).fit_predict(X)\n",
    "\n",
    "# shift the colors\n",
    "prediction = (prediction + 2 ) % 3\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], facecolor='', edgecolors=cmap(prediction), lw=2,  s=380, label='prediction')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=cmap(y), label='truth')\n",
    "plt.legend(loc='upper left')\n",
    "plt.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-Means algorithm works well on convex clusters with similar standard deviations. But it fails on elongated or concave shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(170)\n",
    "X, y = make_blobs(n_samples=300, centers=3,)\n",
    "transformation = [[0.4, -0.8], [-0.4, 0.4]]\n",
    "X_elongated = np.dot(X, transformation)\n",
    "prediction = KMeans(n_clusters=3,).fit_predict(X_elongated)\n",
    "\n",
    "\n",
    "plt.scatter(X_elongated[:, 0], X_elongated[:, 1], c=cmap(prediction))\n",
    "plt.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models\n",
    "\n",
    "The k-Means algorithm is a special case of the __expectation-maximization__ for solving __Gaussian mixture models__.\n",
    "\n",
    "Gaussian mixtures assume that $Pr(X)$ is a mixture of gaussians. Explicetly it assumes \n",
    "\n",
    "$$\n",
    "X = (1 - U) \\cdot X_1 + U \\cdot X_2 \n",
    "$$\n",
    "\n",
    "where $U \\in \\{0, 1\\}$ with a fixed (unkown) probability for either 1 or 0 and $X_1$ and $X_2$ both normally distributed with unkown parameters. \n",
    "\n",
    "The EM-Algorithm tries to find these unkown parameters by approximating a lieklihood and minimizing it.\n",
    "\n",
    "For more details see chapter 8.5 in the book and here\n",
    "\n",
    "https://en.wikipedia.org/wiki/Expectationmaximization_algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gm = GaussianMixture(n_components=3).fit(X_elongated)\n",
    "prediction = gm.predict(X_elongated)\n",
    "plt.scatter(X_elongated[:, 0], X_elongated[:, 1], c=cmap(prediction))\n",
    "plt.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density based methods \n",
    "\n",
    "Clustering methods based on density take a more heuristic approach. Assuming that regions of higher density belong together while points in sparse regions are considered as noise.\n",
    "\n",
    "#### DBSCAN \n",
    "\n",
    "Density-based spatial clustering of applications with noise (DBSCAN) is the most popular density based clustering algorithm. It can also find points in non-convex clusters.\n",
    "\n",
    "Read more here:\n",
    "    https://en.wikipedia.org/wiki/DBSCAN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "\n",
    "X, y = make_circles(n_samples=500, noise=0.05, factor=0.3)\n",
    "\n",
    "prediction_kmeans = KMeans(n_clusters=2).fit_predict(X)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.set_title('k-Means clustering')\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=prediction_kmeans, cmap='winter')\n",
    "ax1.axis('off')\n",
    "\n",
    "prediction_dbscan = DBSCAN(eps=0.2).fit_predict(X)\n",
    "ax2.set_title('DBSCAN clustering')\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=prediction_dbscan, cmap='winter')\n",
    "ax2.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Cluster Performance\n",
    "\n",
    "In the case of unsupervised learning there is no ground truth to which the cluster structure can be compared. \n",
    "\n",
    "Some heuristic has to be applied measure how well a clustering performed. \n",
    "\n",
    "#### Silhuette Coefficent \n",
    "\n",
    "This evaluation ciriterion assumes a clsutering is 'good' if the clusters are dense instead of sparse. \n",
    "\n",
    "Define $a$ as the distance between a single point $x_0$ and all other points in the cluster $S_p$.\n",
    "\n",
    "$$\n",
    "a(x_0) = \\sum_{x_i \\in S_0} \\left\\|x_i - x_0 \\right\\|\n",
    "$$\n",
    "\n",
    "and $b$ as the distance between $x_0$ and all the points in the *nearest cluster* $S_p^\\prime$\n",
    "\n",
    "$$\n",
    "b(x_0) = \\sum_{x_i \\in S_0^\\prime} \\left\\|x_i - x_0 \\right\\|\n",
    "$$\n",
    "\n",
    "The Silhuette Coefficent is then defined as \n",
    "\n",
    "$$\n",
    "s = \\frac{b - a}{\\text{max}(a, b)}\n",
    "$$\n",
    "\n",
    "\n",
    "The coefficent takes a value close to +1 for dense clustering and -1 for sparse clusters. \n",
    "\n",
    "Unfortunately it doesn't work very reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(172)\n",
    "from sklearn.metrics import silhouette_score\n",
    "X, y = make_blobs(n_samples=300, centers=2,)\n",
    "transformation = [[0.4, -0.8], [-0.4, 0.4]]\n",
    "X = np.dot(X, transformation)\n",
    "\n",
    "km = KMeans(n_clusters=2)\n",
    "prediction_kmeans = km.fit_predict(X)\n",
    "score_kmeans = silhouette_score(X, km.labels_ ) \n",
    "\n",
    "\n",
    "db = DBSCAN(eps=0.67)\n",
    "prediction_db = db.fit_predict(X)\n",
    "score_db = silhouette_score(X, db.labels_ ) \n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.set_title('k-Means clustering score: {:0.3f}'.format(score_kmeans))\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=prediction_kmeans, cmap='winter')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.set_title('DBSCAN clustering: {:0.3f}'.format(score_db))\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=prediction_db, cmap='winter')\n",
    "ax2.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=300, noise=0.1)\n",
    "\n",
    "km = KMeans(n_clusters=2)\n",
    "prediction_kmeans = km.fit_predict(X)\n",
    "score_kmeans = silhouette_score(X, km.labels_ ) \n",
    "\n",
    "db = DBSCAN(eps=0.19)\n",
    "prediction_db = db.fit_predict(X)\n",
    "score_db = silhouette_score(X, db.labels_ ) \n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.set_title('k-Means clustering score: {:0.3f}'.format(score_kmeans))\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=prediction_kmeans, cmap='winter')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.set_title('DBSCAN clustering: {:0.3f}'.format(score_db))\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=prediction_db, cmap='winter')\n",
    "ax2.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Material\n",
    "\n",
    "Read scikit-learns's user guid. It's detailed and describes pros and cons of many alogirthms and evaluation criteria.\n",
    "Its also full of code examples.\n",
    "\n",
    "http://scikit-learn.org/stable/user_guide.html\n",
    "\n",
    "Read the book by Hastie (if you're a crazy maths person)\n",
    "\n",
    "http://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "Read this book by James and Hastie (if you're a normal person)\n",
    "\n",
    "http://www-bcf.usc.edu/~gareth/ISL/\n",
    "\n",
    "\n",
    "A nice intro into Deep Learning and Neural Networks\n",
    "\n",
    "http://cs231n.github.io\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
